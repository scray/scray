{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "def adddatecolumns(data,pf) :\n",
    "    data['year'] = pf['CSTARTTIME'].apply(lambda x: x.date().year)\n",
    "    data['month'] = pf['CSTARTTIME'].apply(lambda x: x.date().month)\n",
    "    data['day'] = pf['CSTARTTIME'].apply(lambda x: x.date().day)\n",
    "    data['hour'] = pf['CSTARTTIME'].apply(lambda x: x.time().hour)\n",
    "    data['minute'] = pf['CSTARTTIME'].apply(lambda x: x.time().minute)\n",
    "    data['second'] = pf['CSTARTTIME'].apply(lambda x: x.time().second)\n",
    "    data['microsecond'] = pf['CSTARTTIME'].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "#vis functions\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "            \n",
    "def abc(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a + \"-\" + b + \"-\" + c \n",
    "\n",
    "def abcd(mdcountsall) :\n",
    "    return abc(mdcountsall) + \"-\" + mdcountsall.index.get_level_values(3).astype(str)\n",
    "\n",
    "def ymdh(mdcountsall) :\n",
    "    return abcd(mdcountsall)\n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    return a + \"-\" + b + \"-\" + c + \"-\" + d\n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mdcountsall=pfall[(pfall['month'] == 2) & (pfall['day'] > 16)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall[(pfall['month'] == 2)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "\n",
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ym(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['outcome'] == outcome)].groupby(['year','month'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month'])['outcome'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    #data2['hours'] =  d.astype(int) \n",
    "    #data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def createBarplot(md,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "# kernel density estimate (KDE) \n",
    "def createKDE(data2,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    # Histogram \n",
    "    #ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "    ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Anomaly Detection with LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "OUTCOME = 'close'\n",
    "\n",
    "TIME_STEPS = 24\n",
    "#TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0,2)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "#sc = SparkContext()\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparkSession = SparkSession.builder.config(\"spark.executor.memory\", \"8g\").config(\"spark.driver.memory\", \"8g\").config(\"spark.driver.maxResultSize\", \"0\").appName(\"example-pyspark-read-and-write\").getOrCreate()\n",
    "\n",
    "\n",
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cat_features = ['CSERVICE','CRECEIVERPROTOCOL','CSENDERPROTOCOL','CSENDERENDPOINTID','CRECEIVERENDPOINTID','CSTATUS']\n",
    "\n",
    "def encodeall(pfall,cat_features):\n",
    "    #Prepping categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    # Apply the label encoder to each column\n",
    "    encodedpfall = pfall[cat_features].apply(encoder.fit_transform)\n",
    "    return encodedpfall\n",
    "  \n",
    "def getpfbasic(pfall) :   \n",
    "    #pfall = df2.limit(5000000).toPandas()    \n",
    "    #print(pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME'],len(pfall.index))\n",
    "    pfall = pfall.assign(outcome=(~( ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'InvoicePortal')) | ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'IDS')) | (pfall['CSTATUS'] == 'SUCCESS') | (pfall['CSTATUS'] == 'SUCCESS_DOWNLOADED') | (pfall['CSTATUS'] == 'SUCCESS_POLLQUEUE'))).astype(int))\n",
    "    converttimestampcolumnn(pfall,'CSTARTTIME')\n",
    "    pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "    #pfall['CSENDERENDPOINTID'] = pfall['CSENDERENDPOINTID'].astype(str)   \n",
    "    #pfall['CRECEIVERENDPOINTID'] = pfall['CRECEIVERENDPOINTID'].astype(str)\n",
    "    pfall['CMESSAGETAT2'] = pfall['CMESSAGETAT2'].astype(int)\n",
    "    pfall['CSLATAT'] = pfall['CSLATAT'].astype(int)\n",
    "    pfall['CINBOUNDSIZE'] = pfall['CINBOUNDSIZE'].astype(int)\n",
    "    return pfall\n",
    "\n",
    "def getpf(df2) :    \n",
    "    pfall = getpfbasic(df2)\n",
    "    encodedpfall = encodeall(pfall,cat_features)\n",
    "    dataall = pfall[['CGLOBALMESSAGEID','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE', 'outcome']].join(encodedpfall)\n",
    "    adddatecolumns(dataall,pfall)    \n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "selected = ['CSERVICE','CRECEIVERPROTOCOL','CSENDERPROTOCOL','CSENDERENDPOINTID','CRECEIVERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE','CSTATUS']\n",
    "\n",
    "def getpfall(df,selected) :\n",
    "    #pfall = df.limit(5000000).toPandas()  \n",
    "    pfall = df.toPandas() \n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(str)\n",
    "    if len(pfall) == 0:\n",
    "        return pfall,0,0\n",
    "    return pfall, int(pfall.loc[pfall['CSTARTTIME'].astype(int).idxmax()]['CSTARTTIME']),len(pfall.index)        \n",
    "\n",
    "def gettest(to,selected) :\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME')    \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "def getdata_lt(to,selected) :\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME') \n",
    "    #dataall = getpf(df2)    \n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "    \n",
    "def getdata_gt(_from,_diff,selected) :\n",
    "    to = _from + _diff\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") > _from ).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME') \n",
    "    #dataall = getpf(df2)\n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "start_lt = 1577041508174\n",
    "start_gt = [1577041507723,1580032925655,1581929401680,1583136852562]\n",
    "timestamp_diff = 3000000000\n",
    "\n",
    "dataall2 = getdata_lt(start_lt,selected)\n",
    "dataall = dataall2[0]\n",
    "\n",
    "while True:\n",
    "    dataall2 = getdata_gt(dataall2[1] ,timestamp_diff,selected)\n",
    "    if dataall2[2] == 0:\n",
    "        break  \n",
    "    dataall = dataall.append(dataall2[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = getpf(dataall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,2)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,1)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages with errors sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details for different CSENDERENDPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "TOP=500000\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['outcome'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > TOP].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall1 = pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'])]\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID']==int(topsender.iloc[7]['CSENDERENDPOINTID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = str(len(topsender)) + \" senders with more than \" + str(TOP) + \" messages (kernel density estimate )\"\n",
    "createKDE(data2,16,8,2,title)\n",
    "label(ax,1,90)\n",
    "#topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages of selected endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": number messages so far = \" + str(topsender.iloc[7]['outcome']))\n",
    "        \n",
    "\n",
    "def createHeatmapSeries(pfall, months,category, outcome) :\n",
    "    for month in months:\n",
    "        data2 = createData(pfall,month,outcome)\n",
    "        piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "        #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "        titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "        sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "        createHeatmap(piv, titlestring)\n",
    "       \n",
    "createHeatmapSeries(pfall1,[10,11,1,2,3,4],'messages', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages with errors of selected endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createHeatmapSeries(pfall1, [11,12,1,2,3,4],'messages with errors', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with LSTM Autoencoders (selected SENDERPOINTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDataframe(pfall1)\n",
    "train, test = getTrainAndTest(df,0.95)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 1.5)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies.head()\n",
    "#anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall2 = getdata_gt(1583136852562 ,timestamp_diff,selected)\n",
    "#len(dataall2)\n",
    "\n",
    "#pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME'],len(pfall.index)\n",
    "#pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME']\n",
    "#pfall.dtypes\n",
    "#pfall['CSTARTTIME'].idxmax()\n",
    "\n",
    "#dataall2[2]\n",
    "dataall\n",
    "#dataall.loc[dataall['CSTARTTIME'].astype(int).idxmax()]['CSTARTTIME'],len(dataall.index)\n",
    "\n",
    "#dataall.loc[1765113]\n",
    "#df.head()\n",
    "#dataall\n",
    "#pfall2 = getpf(dataall[(dataall['CSERVICE'] == 'IDS') & (dataall['CSTATUS'] == 'PENDING') ])\n",
    "\n",
    "#pd.unique(dataall[dataall['CSTATUS'] == 'PENDING' ]['CSERVICE' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cgeck for NaN columns\n",
    "na = dataall.isna().any()\n",
    "#dataall\n",
    "status = pd.unique(dataall['CSTATUS'])\n",
    "service = pd.unique(dataall['CSERVICE'])\n",
    "rprotocol = pd.unique(dataall['CRECEIVERPROTOCOL'])\n",
    "sprotocol = pd.unique(dataall['CRECEIVERPROTOCOL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#status\n",
    "service\n",
    "#sprotocol\n",
    "#pfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['CSENDERENDPOINTID','CRECEIVERENDPOINTID']\n",
    "trial = getdata_lt(start_lt,selected)\n",
    "\n",
    "for each in start_gt:\n",
    "    trial = trial.append(getdata_gt(each ,timestamp_diff,selected))\n",
    "           \n",
    "#trial = gettest(1577041508174,['CSENDERENDPOINTID','CRECEIVERENDPOINTID'])\n",
    "#trial\n",
    "#enc = encodeall(trial,['CSENDERENDPOINTID','CRECEIVERENDPOINTID'])\n",
    "#enc\n",
    "\n",
    "#pd.unique(trial['CRECEIVERENDPOINTID'])\n",
    "\n",
    "#ges = pd.DataFrame(trial['CRECEIVERENDPOINTID'] )\n",
    "#ges['receiver'] = encodeall(ges,['CRECEIVERENDPOINTID'])['CRECEIVERENDPOINTID']\n",
    "#ges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall\n",
    "#len(pd.unique(trial['CRECEIVERENDPOINTID'])),len(pd.unique(trial['CSENDERENDPOINTID']))\n",
    "#senders = pd.unique(trial['CSENDERENDPOINTID']) \n",
    "#receivers = pd.unique(trial['CRECEIVERENDPOINTID']) \n",
    "\n",
    "\n",
    "#result = []\n",
    "#result.append(senders)\n",
    "#result.append(receivers)\n",
    "#result = np.concatenate((senders,receivers))\n",
    "#len(pd.unique(result)), len(result),len(senders) , len(receivers)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# determine categorical and numerical features\n",
    "numerical_ix = trial.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_ix = trial.select_dtypes(include=['object', 'bool']).columns\n",
    "# define the data preparation for the columns\n",
    "t = [('cat', OneHotEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)]\n",
    "col_transform = ColumnTransformer(transformers=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial['CSENDERENDPOINTID']\n",
    "from sklearn import preprocessing\n",
    "#trial.columns\n",
    "le = preprocessing.LabelEncoder()\n",
    "X_2 = trial.apply(le.fit_transform)\n",
    "X_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(trial)\n",
    "onehotlabels = enc.transform(trial).toarray()\n",
    "onehotlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotlabels\n",
    "rlist = pd.unique(trial['CRECEIVERENDPOINTID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [rlist,rlist]\n",
    "categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = trial['CRECEIVERENDPOINTID']\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "# binary encode . INSTANTIATE\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "print(integer_encoded)\n",
    "\n",
    "# 2. FIT\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(onehot_encoded)\n",
    "\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)\n",
    "#integer_encoded[1]\n",
    "\n",
    "# 3. Transform\n",
    "#onehotlabels = onehot_encoder.transform(values).toarray()\n",
    "#onehotlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORK\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "data = trial.apply(le.fit_transform)\n",
    "\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0,1])], remainder='passthrough')\n",
    "\n",
    "#ohe = OneHotEncoder([0,1],  sparse=False, handle_unknown=\"ignore\")\n",
    "#df_processed_np = ohe.fit_transform(trial)\n",
    "\n",
    "\n",
    "#data2 = columnTransformer.fit_transform(data)\n",
    "#print(data2)\n",
    "#data\n",
    "#categorical_features = list(trial.columns[trial.dtypes == 'object'])\n",
    "\n",
    "#columnTransformer.fit_transform(data)\n",
    "#dataset = np.array(columnTransformer.fit_transform(data), dtype = np.str)\n",
    "dataset = np.array(columnTransformer.fit_transform(trial), dtype = np.object)\n",
    "type(dataset)\n",
    "\n",
    "columnTransformer.fit_transform(trial).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataall[['CRECEIVERENDPOINTID']].copy()\n",
    "#values.ndim\n",
    "#values\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "hs_train_transformed = ohe.fit_transform(values)\n",
    "hs_train_transformed\n",
    "hs_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ohe.get_feature_names()\n",
    "feature_names\n",
    "\n",
    "row0 = hs_train_transformed[0]\n",
    "row0, feature_names[row0 == 1], values.values[0], ohe.inverse_transform([row0])\n",
    "\n",
    "hs_inv = ohe.inverse_transform(hs_train_transformed)\n",
    "hs_inv\n",
    "np.array_equal(hs_inv, values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant',\n",
    "                   fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False,\n",
    "                    handle_unknown='ignore'))\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "cat_cols = ['CSENDERENDPOINTID', 'CRECEIVERENDPOINTID']\n",
    "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
    "ct = ColumnTransformer(transformers=cat_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_transformed = ct.fit_transform(trial)\n",
    "X_cat_transformed.shape\n",
    "#X_cat_transformed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall = dataall.append(dataall2, ignore_index=True).append(dataall3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/dataall.parquet/*')\n",
    "dataall = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataall to dataframe and store it to hdfs\n",
    "#dataall\n",
    "dfm = sparkSession.createDataFrame(dataall)\n",
    "dfm.write.parquet('hdfs://172.30.17.145:8020/user/admin/dataall_200403.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a pandas DataFrame\n",
    "#pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "dfall = sparkSession.createDataFrame(pfall)\n",
    "dfall.write.parquet('hdfs://172.30.17.145:8020/user/admin/pfall.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pfall[pfall['month'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(pfall.outcome)\n",
    "#dataall\n",
    "#encodedpfall\n",
    "#mdcountsall\n",
    "\n",
    "#mdcountsall = pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "#mdcountsall = pfall[(pfall['month'] == 2) & (pfall['day'] == 20)].groupby(['year','month','day','hour','outcome'])['outcome'].count()\n",
    "#mdcountsall = pfall[(pfall['month'] == 3)].groupby(['year','month','day','outcome'])['outcome'].count()\n",
    "#mdcountsall = pfall.groupby(['year','month','outcome'])['outcome'].count()\n",
    "\n",
    "#mdcountsall = pfall[(pfall['outcome'] == 0) & (pfall['month'] == 3)].groupby(['year','month','day','outcome'])['outcome'].count()\n",
    "\n",
    "mdcountsall = pfall[(pfall['outcome'] == 0) ].groupby(['year','month','outcome'])['outcome'].count()\n",
    "\n",
    "#filter results\n",
    "#mdcountsall = mdcountsall[mdcountsall > 150000]\n",
    "\n",
    "#.reset_index()\n",
    "\n",
    "#& (pfall['outcome'] > 15000)\n",
    "#md = md[md['outcome'] > 15000].reset_index()\n",
    "\n",
    "#mdcountsall = dataall[dataall['outcome'] == 1].groupby(['year','month','day'])['outcome'].count()\n",
    "#mdcountsall = dataall[dataall['outcome'] == 1].groupby(['year','month','day'])['outcome'].count().apply(lambda x: x / 175000 * 3500)\n",
    "\n",
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(26,12))\n",
    "\n",
    "# Add title\n",
    "#plt.title(\"Number of messages with errors (year, month, day)\")\n",
    "#plt.title(\"Number of messages (year, month, day)\")\n",
    "plt.title(\"Number of messages / errors (year, month)\")\n",
    "\n",
    "# Bar chart showing average arrival delay for Spirit Airlines flights by month\n",
    "g = sns.barplot(x=mdcountsall.index, y=mdcountsall)\n",
    "#g.set_xticklabels(g.get_xticklabels(), rotation=70)\n",
    "label(g,1,70)\n",
    "\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Messages\")\n",
    "#mdcountsall.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = pfall[(pfall['outcome'] == 0) & (pfall['month'] == 3)].groupby(['year','month','day','outcome'])['outcome'].count()\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] =   abc(md)\n",
    "#data2['outcome'] =  md.reset_index()['outcome'].astype(int) \n",
    "\n",
    "#plt.figure(figsize=(16,6))\n",
    "\n",
    "#sns.heatmap(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.barplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.scatterplot(x=data2['date'], y=data2['CINBOUNDSIZE'], data=data2)\n",
    "#label(ax,4,90)\n",
    "#data2\n",
    "#md.reset_index()['outcome'].astype(int) \n",
    "md['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataall.groupby(['year','month','day', 'outcome'])['outcome'].count()[2019,11]\n",
    "#dataall.groupby(['year','month','day'])['outcome'].count()\n",
    "\n",
    "md=pfall.groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#md['outcome'] = md['outcome'].astype(int)\n",
    "#md=md[md['outcome']  > '10000']\n",
    "\n",
    "md.index =   ymdh(md)\n",
    "\n",
    "md = md.reset_index()\n",
    "#mdcountsall.index = mdcountsall['level_0']\n",
    "md.head()\n",
    "md = md[md['outcome'] > 15000].reset_index()\n",
    "\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Daily\")\n",
    "\n",
    "# Line chart showing daily global streams of each song \n",
    "#mdcountsall=pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "#mdcountsall.reset_index\n",
    "#g = sns.lineplot(x=md.index, y=md['outcome'])\n",
    "#ax = sns.lineplot(x=md['index'], y=md['outcome'])\n",
    "\n",
    "ax = sns.scatterplot(x=md['index'], y=md['outcome'], data=md)\n",
    "\n",
    "#g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "#sns.lineplot(x=mdcountsall.index, y=mdcountsall)\n",
    "#sns.lineplot(data=mdcountsall)\n",
    "#sns.distplot(pfall.groupby(['year','month','day'])['outcome'].count())\n",
    "\n",
    "# Add label for vertical axis\n",
    "#plt.ylabel(\"Messages\")\n",
    "label(ax,8,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "#sns.heatmap(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.barplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.scatterplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "ax = sns.lineplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "\n",
    "# Histogram \n",
    "#ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "# kernel density estimate (KDE) \n",
    "#ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "\n",
    "#label(ax,80,45)\n",
    "label(ax,5,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = createData(pfall,4,2) \n",
    "piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "createHeatmap(piv)\n",
    "#piv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mdcountsall.head()\n",
    "#a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "#mdcountsall[0,1,2019]\n",
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "#mdcountsall[0,2].count()\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['outcome'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > 500000].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topsender\n",
    "#len(pfall['CSENDERENDPOINTID']==2136)\n",
    "#pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'][0])]\n",
    "#pfall[pfall['CSENDERENDPOINTID']==2137]\n",
    "#pfall.head()\n",
    "\n",
    "#pfall1 = pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'])]\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID']==int(topsender.iloc[7]['CSENDERENDPOINTID'])]\n",
    "#pfall1\n",
    "#topsender.iloc[7]['CSENDERENDPOINTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "month = 4\n",
    "category = 'messages'\n",
    "months = [10,11,1,2,3,4]\n",
    "\n",
    "for month in months:\n",
    "    data2 = createData(pfall,month,2)\n",
    "    piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "    titlestring = \"Number of \"+ category + \": CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \" = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "    createHeatmap(piv, titlestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(get_ymdh_string(pfall['year'].map(str),pfall['month'].map(str),pfall['day'].map(str),pfall['hour'].map(str)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall['CSENDERENDPOINTID'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcountsall = pfall.groupby(['outcome','CSENDERENDPOINTID','year','month','day','hour'])['outcome'].count()\n",
    "\n",
    "pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "\n",
    "#a,b,c,d = get_ymdh(mdcountsall)\n",
    "#data2 = pd.DataFrame()\n",
    "#data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "#data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "#sns.heatmap(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.barplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.scatterplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "#ax = sns.lineplot(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "\n",
    "# Histogram \n",
    "#ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "# kernel density estimate (KDE) \n",
    "ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "\n",
    "#label(ax,80,45)\n",
    "label(ax,1,90)\n",
    "\n",
    "\n",
    "topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection with LSTM Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install\n",
    "import os\n",
    "os.environ['http_proxy'] = \"httpp://172.30.12.56:3128\" \n",
    "os.environ['https_proxy'] = \"https://172.30.12.56:3128\"   \n",
    "!pip install gdown\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "!gdown --id 10vdMg_RazoIatwrT7azKFX4P02OebU76 --output spx.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "df = pd.read_csv('spx.csv', parse_dates=['date'], index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old trial\n",
    "type(data2)\n",
    "df = pd.DataFrame()\n",
    "df['close'] = data2['outcome']\n",
    "df.set_index(data2['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3\n",
    "#pfall['CMESSAGETAT2'].head()\n",
    "#pfall.head()\n",
    "#data3 = createDataAno(outcome)\n",
    "df = pd.DataFrame()\n",
    "df['close'] = pfall['CMESSAGETAT2']\n",
    "#df.set_index(data3['date'], inplace=True)\n",
    "df.set_index(get_ymdh_string(pfall['year'].map(str),pfall['month'].map(str),pfall['day'].map(str),pfall['hour'].map(str)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "df = df.groupby(df.index).sum()\n",
    "df.head()\n",
    "\n",
    "#len(df) / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing : plot\n",
    "plt.plot(df, label='close price')\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Anomaly Detection with LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "OUTCOME = 'close'\n",
    "\n",
    "TIME_STEPS = 24\n",
    "#TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0,2)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDataframe(pfall)\n",
    "train, test = getTrainAndTest(df,0.95)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "#model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 1.5)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#anomalies.head()\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del dataall['microsecond']\n",
    "#pfall.dtypes\n",
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training, validation, and test splits (V1)\n",
    "\n",
    "#data = pfall[(pfall['month'] == 11)]\n",
    "data = pfall\n",
    "\n",
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "test = data[-valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training, validation, and test splits (V2 - current)\n",
    "data = pfall\n",
    "train, valid, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data\",len(data),\"train\",len(train),\"valid\",len(valid),\"test\",len(test))\n",
    "\n",
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction  = {each.outcome.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a LightGBM model\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome').drop('CSTATUS').drop('CGLOBALMESSAGEID').drop('microsecond')\n",
    "#feature_cols = feature_cols.drop('CSENDERENDPOINTID')\n",
    "#feature_cols = feature_cols.drop('CRECEIVERENDPOINTID')\n",
    "##feature_cols = feature_cols.drop('year')\n",
    "#feature_cols = feature_cols.drop('month')\n",
    "#feature_cols = feature_cols.drop('day')\n",
    "#feature_cols = feature_cols.drop('hour')\n",
    "feature_cols = feature_cols.drop('minute')\n",
    "feature_cols = feature_cols.drop('second')\n",
    "feature_cols = feature_cols.drop('CSLATAT')\n",
    "feature_cols = feature_cols.drop('CMESSAGETAT2')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions & evaluating the model\n",
    "\n",
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getres():\n",
    "    res = test[feature_cols].copy()\n",
    "    res['ypred'] = ypred\n",
    "    res = res.assign(ypred=((res['ypred'] >0.5) ).astype(int))\n",
    "    res['outcome'] = test['outcome']\n",
    "    res['CSTATUS'] = test['CSTATUS']\n",
    "    return res\n",
    "\n",
    "res = getres()\n",
    "#res\n",
    "\n",
    "wrong = (res[(res['outcome'] != res['ypred'] )])\n",
    "right = (res[(res['outcome'] == res['ypred'] )])\n",
    "\n",
    "#len((res[(res['outcome'] != res['ypred'] )])) , len((res[res['outcome'] == res['ypred'] ]))\n",
    "\n",
    "\n",
    "#t = res['outcome'] \n",
    "#len(t[t == 1]), len(s[s == 1])\n",
    "\n",
    "#len(wrong),len(wrong[wrong['outcome'] == 0]),len(wrong[wrong['outcome'] == 1]),len(right),len(right[right['outcome'] == 0]),len(right[right['outcome'] == 1]),right[right['outcome'] == 1].groupby(['CSTATUS'])['outcome'].count()\n",
    "\n",
    "print(feature_cols)\n",
    "print(\"data\",len(data),\"train\",len(train),\"valid\",len(valid),\"test\",len(test))\n",
    "\n",
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction  = {each.outcome.mean():.4f}\")\n",
    "\n",
    "print(f\"Test AUC score: {score}\")    \n",
    "    \n",
    "print (\"prediction wrong:\", len(wrong), \"prediction right:\",len(right)) \n",
    "print(\"right(success):\" ,len(right[right['outcome'] == 0]),\"right(error):\", len(right[right['outcome'] == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[feature_cols]\n",
    "\n",
    "mdcountsall=test[feature_cols].groupby(['year','month','day'])['CINBOUNDSIZE'].count()\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] =   abc(mdcountsall)\n",
    "data2['CINBOUNDSIZE'] =  mdcountsall.reset_index()['CINBOUNDSIZE'].astype(int) \n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "#sns.heatmap(x=data2['date'], y=data2['outcome'], data=data2)\n",
    "ax = sns.barplot(x=data2['date'], y=data2['CINBOUNDSIZE'], data=data2)\n",
    "#ax = sns.scatterplot(x=data2['date'], y=data2['CINBOUNDSIZE'], data=data2)\n",
    "label(ax,4,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.iloc[0], \n",
    "#ypred[1] < 0.5\n",
    "#len(ypred[ypred > 0.5]),len(test[test['outcome'] == 1])\n",
    "#ypred[ypred > 0.5]\n",
    "#test\n",
    "#feature_cols\n",
    "#res = test[feature_cols]\n",
    "#res\n",
    "#len(res.index), len(ypred)\n",
    "#res['ypred'] = ypred\n",
    "#res = res.assign(ypred=((res['ypred'] >0.5) ).astype(int))\n",
    "#res['outcome'] = test['outcome']\n",
    "#res[res['outcome']==1]\n",
    "\n",
    "#res.outcome.mean(), res.ypred.mean()\n",
    "#len((res[~(res['outcome'] == res['ypred'] )])) , len((res[res['outcome'] == res['ypred'] ]))\n",
    "#(res[~(res['outcome'] == res['ypred'] )])\n",
    "#s = (res[~(res['outcome'] == res['ypred'] )])['outcome'] \n",
    "#s[s == 1]\n",
    "#t = res['outcome'] \n",
    "#len(t[t == 1]), len(s[s == 1])\n",
    "train.columns\n",
    "mdcountsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()\n",
    "label_column = 'outcome'\n",
    "print(\"Summary of class variable: \\n\", train[label_column].describe())\n",
    "dir = 'agModels-predictClass' # specifies folder where to store trained models\n",
    "predictor = task.fit(train_data=train, label=label_column, output_directory=dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "#SparkConf().set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "#df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/1580137124017/part-00000-b2678865-5b2e-43a7-8428-117b6f435dc5-c000.snappy.parquet')\n",
    "\n",
    "#df.show()\n",
    "#pf = df.toPandas()\n",
    "\n",
    "#df = sqlContext.read.parquet(\"/home/jovyan/work/notebook/flat/000000_1\")\n",
    "\n",
    "#df2 = df.select('CSENDERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CSTATUS').limit(30000000).dropDuplicates().orderBy('CSTARTTIME')\n",
    "\n",
    "#df2 = df.select('CSENDERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CSTATUS').filter(df[\"CSTARTTIME\"]>='1577071831179').limit(30000000).dropDuplicates().orderBy('CSTARTTIME')\n",
    "\n",
    "#1580427009923\n",
    "#df2 = df.select('CSENDERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CSTATUS').filter(df[\"CSTARTTIME\"]>='1580427009923').limit(30000000).dropDuplicates().orderBy('CSTARTTIME')\n",
    "\n",
    "df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).select('CSENDERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CSTATUS').filter(col(\"CSTARTTIME\")>=1580427009923)\n",
    "#.dropDuplicates().orderBy('CSTARTTIME')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()\n",
    "#pfall.loc[pfall['CSTARTTIME'].astype(str).astype(long).idxmax()]\n",
    "#pfall.loc[pfall['CSTARTTIME'].idxmax()]\n",
    "pfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall\n",
    "#pd.unique(pfall.CSTATUS)\n",
    "#pf.groupby('CSTATUS')['CGLOBALMESSAGEID'].count()\n",
    "#pfall = pfall.assign(outcome=(pfall['CSTATUS'] == 'SUCCESS').astype(int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pf = df.limit(1000000).toPandas()\n",
    "#pf=df.filter(df[\"CSERVICE\"]=='PEPPOL').select('*').limit(1000000).dropDuplicates().toPandas()\n",
    "#pf=df.filter(df[\"CSERVICE\"]=='PEPPOL').select('*').limit(1000000).toPandas()\n",
    "#df.head()\n",
    "\n",
    "df1 = df.filter(df[\"CSTATUS\"]!='SUCCESS').filter(df[\"CSTATUS\"]!='SUCCESS_DOWNLOADED').select('CGLOBALMESSAGEID','CSTARTTIME','CENDTIME','CSTATUS','CSENDERPROTOCOL','CSENDERENDPOINTID','CINBOUNDSIZE','CRECEIVERPROTOCOL','CRECEIVERENDPOINTID','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CSERVICE').limit(30000000).dropDuplicates().orderBy('CSTARTTIME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df2.limit(5000000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.show(1)\n",
    "pf = df1.limit(1000000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1 = df.filter(df[\"CSERVICE\"]=='PEPPOL').select('CGLOBALMESSAGEID','CSTARTTIME','CENDTIME','CSTATUS','CSENDERPROTOCOL','CSENDERENDPOINTID','CINBOUNDSIZE','CRECEIVERPROTOCOL','CRECEIVERENDPOINTID','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME').limit(1000000).dropDuplicates()\n",
    "pf = pf1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pf.sort_values('CGLOBALMESSAGEID', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_datetime(df['CENDTIME']).apply(lambda x: x.date())\n",
    "import datetime\n",
    "readable = datetime.datetime.fromtimestamp(1580983470).isoformat()\n",
    "print(readable)\n",
    "\n",
    "ab = pf.iloc[121]['CENDTIME']\n",
    "\n",
    "import datetime\n",
    "date = datetime.datetime.fromtimestamp(float(ab) / 1e3)\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to datetime\n",
    "import datetime\n",
    "pf['CENDTIME'] = pf['CENDTIME'].apply(lambda x: datetime.datetime.fromtimestamp(float(x) / 1e3))\n",
    "pf['CSTARTTIME'] = pf['CSTARTTIME'].apply(lambda x: datetime.datetime.fromtimestamp(float(x) / 1e3))\n",
    "pf['CSLADELIVERYTIME'] = pf['CSLADELIVERYTIME'].apply(lambda x: datetime.datetime.fromtimestamp(float(x) / 1e3))\n",
    "\n",
    "pf['CINBOUNDSIZE'] = pf['CINBOUNDSIZE'].astype(str).astype(int)\n",
    "pf['CMESSAGETAT2'] = pf['CMESSAGETAT2'].astype(str).astype(int)\n",
    "pf['CSLATAT'] = pf['CSLATAT'].astype(str).astype(int)\n",
    "\n",
    "pf['CSENDERPROTOCOL'] = pf['CSENDERPROTOCOL'].astype(str)\n",
    "pf['CSERVICE'] = pf['CSERVICE'].astype(str)\n",
    "pf['CRECEIVERPROTOCOL'] = pf['CRECEIVERPROTOCOL'].astype(str)\n",
    "\n",
    "pf['CSENDERENDPOINTID'] = pf['CSENDERENDPOINTID'].astype(str)\n",
    "pf['CRECEIVERENDPOINTID'] = pf['CRECEIVERENDPOINTID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pf[\"CSTARTTIME\"] = pd.to_numeric(pf[\"CSTARTTIME\"])\n",
    "#set(pf['CSTARTTIME'].dt.date)\n",
    "pf\n",
    "#pf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expired = pf[pf['CSTATUS']=='EXPIRED']\n",
    "expired = expired.sort_values('diff', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = pf[pf['CSTATUS']=='SUCCESS']\n",
    "success['diff'] = (success.CENDTIME - success.CSTARTTIME).dt.total_seconds()\n",
    "success = success.sort_values('diff', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success.iloc[1]\n",
    "#expired.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pf.CENDTIME-pf.CSTARTTIME).astype('timedelta64[s]')\n",
    "\n",
    "def diffdt(el):\n",
    "    return (el.CENDTIME-el.CSTARTTIME).total_seconds(),el.CSTARTTIME,el.CENDTIME,el\n",
    "\n",
    "(pf.CENDTIME-pf.CSTARTTIME).dt.total_seconds()\n",
    "el = expired.iloc[0]\n",
    "el.CENDTIME, el.CSTARTTIME,(el.CENDTIME - el.CSTARTTIME).total_seconds() \n",
    "\n",
    "#diffdt(el)\n",
    "#expired['diff'] = (expired.CENDTIME - expired.CSTARTTIME).dt.total_seconds()\n",
    "expired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "import numpy as np\n",
    "pandas_profiling.ProfileReport(dataall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "\n",
    "def get_categories(column):\n",
    "    return np.array(df.select(column).dropDuplicates().dropna().collect())\n",
    "\n",
    "services = get_categories('CSERVICE')\n",
    "receivers = get_categories('CRECEIVERENDPOINTID')\n",
    "senders = get_categories('CSENDERENDPOINTID')\n",
    "receiverprotocols = get_categories('CRECEIVERPROTOCOL')\n",
    "senderprotocols = get_categories('CSENDERPROTOCOL')\n",
    "status = get_categories('CSTATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#services\n",
    "#status\n",
    "pd.unique(pf.CSTATUS)\n",
    "pf.groupby('CSTATUS')['CGLOBALMESSAGEID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = pd.unique(pf.CSENDERENDPOINTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['CSTATUS']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded2 = pf[cat_features].apply(encoder.fit_transform)\n",
    "#encoded2.head(10)\n",
    "\n",
    "data['CSTATUS'] = encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['CSENDERPROTOCOL', 'CSERVICE', 'CRECEIVERPROTOCOL', 'CSENDERENDPOINTID', 'CRECEIVERENDPOINTID']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded = pf[cat_features].apply(encoder.fit_transform)\n",
    "encoded.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "pf.dtypes\n",
    "\n",
    "#datetime.datetime.fromtimestamp(pf['CSTARTTIME'][0])\n",
    "\n",
    "ti = pf['CSTARTTIME'][0].time()\n",
    "\n",
    "\n",
    "ti.hour ,ti.minute,ti.second, ti.microsecond, pf['CSTARTTIME'][0]\n",
    "\n",
    "\n",
    "pf['CSTARTTIME'].apply(lambda x: x.date().month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CMESSAGETAT2 : Laufzeit Msg (ms) : end - start\n",
    "\n",
    "data = pf[['CMESSAGETAT2', 'CSLATAT', 'CINBOUNDSIZE']].join(encoded)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = pf['CSTARTTIME'].apply(lambda x: x.date().year)\n",
    "data['month'] = pf['CSTARTTIME'].apply(lambda x: x.date().month)\n",
    "data['day'] = pf['CSTARTTIME'].apply(lambda x: x.date().day)\n",
    "\n",
    "data['hour'] = pf['CSTARTTIME'].apply(lambda x: x.time().hour)\n",
    "data['minute'] = pf['CSTARTTIME'].apply(lambda x: x.time().minute)\n",
    "data['second'] = pf['CSTARTTIME'].apply(lambda x: x.time().second)\n",
    "data['microsecond'] = pf['CSTARTTIME'].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.iloc[0],pf.iloc[0]\n",
    "pd.unique(data.month)\n",
    "#data.groupby('month')['month'].count()\n",
    "#data.groupby(['month','day'],as_index=False).count()['day']\n",
    "mdcounts = data.groupby(['year','month','day'])['CMESSAGETAT2'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdcounts[2020,1]\n",
    "mdcounts.index\n",
    "type(mdcounts)\n",
    "mdcounts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width and height of the figure\n",
    "plt.figure(figsize=(24,6))\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Number of messages with errors (year, month, day)\")\n",
    "\n",
    "# Bar chart showing average arrival delay for Spirit Airlines flights by month\n",
    "g = sns.barplot(x=mdcounts.index, y=mdcounts)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)\n",
    "\n",
    "# Add label for vertical axis\n",
    "plt.ylabel(\"Errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line chart showing daily global streams of each song \n",
    "#sns.lineplot(data=mdcounts)\n",
    "\n",
    "plt.figure(figsize=(34,6))\n",
    "\n",
    "df = pd.DataFrame({\"Datum\": ['1/1/2018 0:00',\n",
    "                             '1/1/2018 0:15',\n",
    "                             '1/1/2018 0:30',\n",
    "                             '1/1/2018 0:45',\n",
    "                             '1/1/2018 1:00',\n",
    "                             '1/1/2018 1:15',\n",
    "                             '1/1/2018 1:30',\n",
    "                             '1/1/2018 1:45 '],\n",
    "                   \"Menge\": [19.5, 19.,19.5,19.5,21,19.5,20,23]})\n",
    "\n",
    "#sns.lineplot(x=\"Datum\", y=\"Menge\", data=df)\n",
    "\n",
    "ar=[2,3,3,3,3]\n",
    "\n",
    "#mdcounts.head()\n",
    "#sns.lineplot(x=pf.head().CSTARTTIME,y=data.CINBOUNDSIZE)\n",
    "#sns.lineplot(x=ar,y=data.head().CINBOUNDSIZE)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from pyhive import hive\n",
    "\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.schema import *\n",
    "\n",
    "#engine = create_engine('hive://clspromon-aio01:10000/default')\n",
    "engine = create_engine('hive://172.30.17.145:10000/default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = engine.execute(\"select * from tables_3_stage1 limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [] \n",
    "for _row in row:\n",
    "    #print(_row['name'])\n",
    "    col.append(_row['name'])\n",
    "col    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector = inspect(engine)\n",
    "\n",
    "# Get table information\n",
    "#print(inspector.get_table_names())\n",
    "\n",
    "# Get column information\n",
    "row=inspector.get_columns('tables_stage1_p_meta_complete')\n",
    "\n",
    "col = [] \n",
    "for _row in row:\n",
    "    #print(_row['name'])\n",
    "    col.append(_row['name'])\n",
    "\n",
    "#col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "row =  engine.execute(\"select * from  tables_stage1_p_meta_complete\")\n",
    "meta_p = pd.DataFrame(row)\n",
    "\n",
    "meta_p.columns = col\n",
    "#pyspark.createDataFrame(meta_p).collect()  \n",
    "\n",
    "meta_p.set_index('metricid', inplace=True)\n",
    "#meta_p.set_index('metricid')\n",
    "\n",
    "meta_p['metricid'] = meta_p.index\n",
    "\n",
    "#meta_p.instance.fillna(value=pd.np.nan, inplace=True)\n",
    "meta_p['instance'].replace(pd.np.nan, \"\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_p.loc[-415304450].__name__\n",
    "#meta_p.iloc[1].service\n",
    "#meta_p\n",
    "#meta_p.loc[meta_p.index[0]].__name__\n",
    "#meta_p.index[1]\n",
    "#meta_p.iloc[1]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_p.info()\n",
    "dfm = sqlContext.createDataFrame(meta_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "\n",
    "#objects = dfm.select('__name__','instance','product', 'metricid', 'schemaid').dropDuplicates()\n",
    "services = np.array(dfm.select('service').dropDuplicates().dropna().collect())\n",
    "names = np.array(dfm.select('__name__').dropDuplicates().dropna().collect())\n",
    "instances = np.array(dfm.select('instance').dropDuplicates().dropna().collect())\n",
    "products = np.array(dfm.select('product').dropDuplicates().dropna().collect())\n",
    "schemaids = np.array(dfm.select('schemaid').dropDuplicates().dropna().collect())\n",
    "metricid = np.array(dfm.select('metricid').dropDuplicates().dropna().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roles = np.array(dfm.select('role').dropDuplicates().dropna().collect())\n",
    "#roles\n",
    "schemaids.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(str(metricid[1][0]))\n",
    "#names.size\n",
    "#dfm.select('__name__').dropDuplicates().dropna().filter(\"__name__ like '%task%'\").collect()\n",
    "\n",
    "#.filter(dfm.__name__.contains('%b%')).show()\n",
    "#dfm.filter(dfm[\"__name__\"]=='bis_inbox_tasks_running').select(\"*\").collect()\n",
    "#dfm.filter(dfm[\"instance\"]=='clspromsg-ts01:13000').select(\"__name__\").collect()\n",
    "#dfm.filter(dfm[\"instance\"]=='clspromsg-ids01:13000').select(\"__name__\").collect()\n",
    "#dfm.filter(dfm[\"__name__\"]=='bis_jms_message_count').select(\"instance\",'metricid').collect()\n",
    "\n",
    "#la = ['__name__','instance','metricid','schemaid','job','alertname','alertstate','severity' ]\n",
    "#dfm.filter(dfm[\"__name__\"]=='bis_jms_message_count').filter(dfm[\"queue\"]=='DtFTPComponent').filter(dfm[\"instance\"]=='clspromsg-ts01:13000').select(la).collect()\n",
    "\n",
    "#alertes = dfm.filter(dfm[\"__name__\"]=='ALERTS').select(la).limit(200000).toPandas()\n",
    "#alertes.schemaid.size\n",
    "#instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = ['__name__','instance','metricid','schemaid','job','alertname','alertstate','severity' ]\n",
    "\n",
    "alerts = dfm.filter(dfm[\"__name__\"]=='ALERTS').select(la).limit(200000).toPandas()\n",
    "alerts.schemaid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"hashvalue\"]==id).select('value').distinct().count()\n",
    "\n",
    "#pf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _row in metricid:\n",
    "    id = str(_row[0])\n",
    "    print(id,df.filter(df[\"hashvalue\"]==id).select('value').distinct().count())\n",
    "\n",
    "id = '-1483665529'\n",
    "#id = str(meta_p.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metricid[0]\n",
    "\n",
    "id = 1797174388 \n",
    "#id = '-415304450'\n",
    "\n",
    "#id = -1756241224 #1\n",
    "#id = 1843265645 #24497\n",
    "#id = 852430459 #1\n",
    "#id = -426467024 #22515\n",
    "#id = 6557551 #25097\n",
    "#id = -1466543823 #26614\n",
    "#id = 612749002 #2\n",
    "#id = -547512297 #24905\n",
    "#id = -415304450 #4920\n",
    "#id = 857353913 #1\n",
    "#id = 500298778 #1\n",
    "#id = -349732210 #2\n",
    "#id = 1786297729 #26414\n",
    "\n",
    "#pf=df.filter(df[\"value\"]>'0').filter(df[\"hashvalue\"]=='2012513398').select('date','value').limit(200000).toPandas()\n",
    "#pf=df.filter(df[\"value\"]>'0').filter(df[\"hashvalue\"]==str(metricid[0])).select('date','value').limit(200000).toPandas()\n",
    "#pf=df.filter(df[\"hashvalue\"]==str(metricid[1000][0])).select('date','value').limit(200000).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pf(id):\n",
    "    pf=df.filter(df[\"hashvalue\"]==str(id)).select('date','value').limit(200000).toPandas()\n",
    "    pf = pf.sort_values('date', ascending=True)\n",
    "    pf[\"value\"] = pd.to_numeric(pf[\"value\"])\n",
    "    return pf\n",
    "\n",
    "def get_plot_title(id) :\n",
    "    title = str(id) + \" = \" + meta_p.loc[id].__name__ + \" -> \" + meta_p.loc[id].instance\n",
    "    return title\n",
    "\n",
    "id = alerts.metricid[3]\n",
    "id=612749002\n",
    "\n",
    "pf = get_pf(id)\n",
    "#alertes.metricid.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=612749002\n",
    "#val = df.filter(df[\"hashvalue\"]==str(id)).select('value')\n",
    "\n",
    "#val.distinct().collect()\n",
    "#val.distinct().count()\n",
    "\n",
    "#val.groupBy('value').count().show()\n",
    "\n",
    "df.filter(df[\"hashvalue\"]==str(id)).filter(df[\"value\"]=='8').select(\"*\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_title = get_plot_title(id)\n",
    "\n",
    "pf.plot(x='date', y='value', title=plot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()\n",
    "pf = pf.sort_values('date', ascending=True)\n",
    "pf.count()\n",
    "#pf = pf.astype('float')\n",
    "pf[\"value\"] = pd.to_numeric(pf[\"value\"])\n",
    "print(type(pf['value'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#df.select(\"hashvalue\", F.when(df.hashvalue == 266655713, 1).otherwise(0)).show()\n",
    "#df.select(\"hashvalue\", F.when(df.hashvalue == 266655713, 1).otherwise(0)).show()\n",
    "#df.select(df.hashvalue.between(266655713,266655713)).show()\n",
    "#df.filter(df[\"hashvalue\"]==str(metricid[1][0])).select('date','hashvalue').rdd.flatMap(list).collect()\n",
    "df.filter(df[\"hashvalue\"]==str(metricid[1][0])).select(\"*\").limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metricid[0][0])\n",
    "len(metricid)\n",
    "\n",
    "for row in products:\n",
    "    print(row[0])\n",
    "\n",
    "for id in metricid:\n",
    "    pf=df.filter(df[\"value\"]>'0').filter(df[\"hashvalue\"]==id[0]).select('date','value').limit(200000).toPandas()\n",
    "    pf.describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.count()\n",
    "products.show()\n",
    "\n",
    "for row in services.rdd.collect():\n",
    "    print(row.service)\n",
    "    \n",
    " \n",
    "#np.array(services.select(\"service\").collect())\n",
    "services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = [] \n",
    "for _row in row:\n",
    "    print(_row['hashvalue'])\n",
    "    hashes.append(_row['hashvalue'])\n",
    "\n",
    "#col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names.select(\"*\").collect()\n",
    "#hashes\n",
    "meta_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "os.environ['https_proxy'] = \"https://172.30.12.56:3128\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade mxnet\n",
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge pillow=6.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install facets-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "facets_path = os.path.dirname('.')\n",
    "facets_path = os.path.abspath(os.path.join(facets_path, 'facets', 'facets_overview', 'python'))\n",
    "                              \n",
    "if not facets_path in sys.path:\n",
    "    sys.path.append(facets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import zipfile\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers for loading/viewing image data\n",
    "\n",
    "# Background color for figures/subplots when showing favicons. Use a faint grey\n",
    "# instead of the default white to make it clear which sections are transparent.\n",
    "BG = '.95'\n",
    "def show(df, scale=1, titles=None):\n",
    "    \"\"\"Show the favicons in the given dataframe, arranged in a grid.\n",
    "    scale is a multiplier on the size of the drawn icons.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    cols = int(min(n, max(4, 8//scale)))\n",
    "    rows = math.ceil(n / cols)\n",
    "    row_height = 1 * scale\n",
    "    col_width = 1 * scale\n",
    "    fs = (cols*col_width, rows*row_height)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=fs, facecolor=BG)\n",
    "    if rows == cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    for i, (row, ax) in enumerate(\n",
    "        itertools.zip_longest(df.itertuples(index=False), axes.flatten())\n",
    "    ):\n",
    "        if row is None:\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            try:\n",
    "                img = load_favicon(row.fname, row.split_index)\n",
    "                _show_img(img, ax)\n",
    "                if titles is not None:\n",
    "                    ax.set_title(titles[i])\n",
    "            except CorruptFaviconException:\n",
    "                ax.axis('off')\n",
    "                \n",
    "def _show_img(img, ax=None):\n",
    "    if ax is None:\n",
    "        _fig, ax = plt.subplots(facecolor=BG)\n",
    "    ax.tick_params(which='both', \n",
    "                   bottom=False, top=False, left=False, right=False,\n",
    "                   labelbottom=False, labeltop=False, labelleft=False, labelright=False,\n",
    "                  )\n",
    "    ax.grid(False, which='both')\n",
    "    plt.setp(list(ax.spines.values()), color='0.8', linewidth=1, linestyle='-')\n",
    "    ax.set_facecolor(BG)\n",
    "    cmap = None\n",
    "    if img.mode in ('L', 'LA'):\n",
    "        cmap = 'gray'\n",
    "    ax.imshow(img, cmap=cmap, aspect='equal', interpolation='none')\n",
    "                \n",
    "class CorruptFaviconException(Exception): pass\n",
    "\n",
    "_ZIP_LOOKUP = {}\n",
    "def load_favicon(fname, split_ix):\n",
    "    if split_ix not in _ZIP_LOOKUP:\n",
    "        zip_fname = '../input/full-{}.z'.format(split_ix)\n",
    "        _ZIP_LOOKUP[split_ix] = zipfile.ZipFile(zip_fname)\n",
    "    archive = _ZIP_LOOKUP[split_ix]\n",
    "    fp = archive.open(fname)\n",
    "    try:\n",
    "        fav = PIL.Image.open(fp)\n",
    "    except (ValueError, OSError):\n",
    "        raise CorruptFaviconException\n",
    "    if fav.format == 'ICO' and len(fav.ico.entry) > 1:\n",
    "        pil_ico_hack(fav)\n",
    "    return fav\n",
    "\n",
    "def pil_ico_hack(img):\n",
    "    \"\"\"When loading an ICO file containing multiple images, PIL chooses the\n",
    "    largest. We want whichever one is listed first.\"\"\"\n",
    "    ico = img.ico\n",
    "    ico.entry.sort(key = lambda d: d['offset'])\n",
    "    first = ico.frame(0)\n",
    "    first.load()\n",
    "    img.im = first.im\n",
    "    img.mode = first.mode\n",
    "    img.size = first.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_df():\n",
    "    \"\"\"Return a dataframe with a row of metadata for each favicon in the dataset.\"\"\"\n",
    "    csvpath = '../input/favicon_metadata.csv'\n",
    "    return pd.read_csv(csvpath)\n",
    "\n",
    "METADATA_CSV = pf\n",
    "METADATA_CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2017)\n",
    "\n",
    "indices = METADATA_CSV.index\n",
    "indices = np.random.choice(indices, size=100000)\n",
    "_metadata_csv = METADATA_CSV.loc[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/opt/facets/facets_overview/python')\n",
    "\n",
    "from generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n",
    "proto = GenericFeatureStatisticsGenerator().ProtoFromDataFrames([{'name': 'Metadata', 'table': _metadata_csv}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import base64\n",
    "protostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-jupyter.html\" >\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the Dive visualization for the training data.\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "jsonstr = pf.to_json(orient='records')\n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-jupyter.html\" >\n",
    "        <facets-overview id=\"elem\"></facets-overview>\n",
    "        <script>\n",
    "          var data = {jsonstr};\n",
    "          document.querySelector(\"#elem\").data = data;\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn.apionly as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create dataframe with datetime as index and aggregated (frequency) values\n",
    "date = pd.date_range('2017-02-23', periods=10*12, freq='2h')\n",
    "freq = np.random.poisson(lam=2, size=(len(date)))\n",
    "df = pd.DataFrame({\"freq\":freq}, index=date)\n",
    "\n",
    "# add a column hours and days\n",
    "df[\"hours\"] = df.index.hour\n",
    "df[\"days\"] = df.index.map(lambda x: x.strftime('%b-%d'))     \n",
    "# create pivot table, days will be columns, hours will be rows\n",
    "piv = pd.pivot_table(df, values=\"freq\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "#plot pivot table as heatmap using seaborn\n",
    "ax = sns.heatmap(piv, square=True)\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=90 )\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
