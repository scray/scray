{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfBasics\n",
    "import common\n",
    "import encoder\n",
    "import pfAdapt\n",
    "import charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "       'CSLABILLINGMONTH', 'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "       'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "       'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "# withot 'CSLABILLINGMONTH'\n",
    "def get_columns_2():\n",
    "    columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "            'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "           'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "           'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "    return columns\n",
    "columns = get_columns_2()\n",
    "#to count messages sent\n",
    "#columns = [ 'CSTARTTIME', 'CSENDERENDPOINTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = dfBasics.getSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*').select(columns).dropDuplicates()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "#df = sparkSession.read.parquet('/tmp/sla_1580137124017.parquet')\n",
    "#df = sparkSession.read.parquet('/tmp/sla/sla_1583136848057.parquet')\n",
    "#!ls /tmp/sla/sla_1583136848057.parquet\n",
    "#df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/sla/sla_1581517910631.parquet')\n",
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/sla/sla_*.parquet').select(['CGLOBALMESSAGEID', 'year', 'month', 'day', 'hour']).filter(col('CSENDERENDPOINTID')==0)\n",
    "\n",
    "\n",
    "#df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/sla/sla_*.parquet').limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(pfall['CGLOBALMESSAGEID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = list(df.select(['CSENDERENDPOINTID']).dropDuplicates().toPandas()['CSENDERENDPOINTID']) \n",
    "#list(df4['CSENDERENDPOINTID'])\n",
    "#encoder.getEncoder('CSENDERENDPOINTID').inverse_transform(senders[100])\n",
    "\n",
    "#len(senders)\n",
    "#senders[2460]\n",
    "#2459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/159041*/*').select(columns).dropDuplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.limit(1).toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/user/admin/CSENDERENDPOINTID/CSENDERENDPOINTID_9.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(pfall['CGLOBALMESSAGEID'])),len(pfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcolumnSeries(column):\n",
    "    return df.select(column).dropDuplicates()\n",
    "def dfcolumnCount(column):\n",
    "    return dfcolumnSeries(column).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def eval1(filteredrows):\n",
    "    element = staticfeatureset.index[0]\n",
    "    res = filteredrows[element].eq(staticfeatureset[element])\n",
    "\n",
    "    for element in staticfeatureset.index:\n",
    "        res = reduce(operator.and_,(res,filteredrows[element].eq(staticfeatureset[element])))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out variable columns and type of value\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected = ['timestamp', 'value','year','month', 'day', 'hour', 'minute']\n",
    "\n",
    "def getDFvalueWithHash(currenthash):\n",
    "    pfm = df.filter(col(\"hashvalue\") == currenthash) \\\n",
    "            .withColumn('value', col('value').cast('int'))\\\n",
    "            .select(selected).toPandas().sort_values('timestamp').reset_index() \n",
    "    return pfm\n",
    "\n",
    "def getDFvalueWithHashSchema(schemaid,currenthash):\n",
    "    pfm = df.filter(col(\"hashvalue\") == currenthash) \\\n",
    "            .filter(col(\"schemaid\") == schemaid) \\\n",
    "            .withColumn('value', col('value').cast('int'))\\\n",
    "            .select(selected).toPandas().sort_values('timestamp').reset_index() \n",
    "    return pfm\n",
    "\n",
    "def getIgroupHashes(igroup):\n",
    "        return df.filter(col(\"igroup\") == igroup).select([\"hashvalue\"]).dropDuplicates().toPandas()['hashvalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns which are variable to find related metrics\n",
    "def getStaticFeatures(_hash):\n",
    "    allcolumns = filteredrows[filteredrows['hashvalue'] == _hash].columns\n",
    "    skipcolumns = ['timestamp', 'value',  'hashvalue', 'igroup', 'inode', 'year', 'month', 'day', 'hour', 'minute']\n",
    "    featurecolumns = allcolumns[~allcolumns.isin(skipcolumns)]\n",
    "    featurecolumns = pfAdapt.getVariableUniqueColums(filteredrows[featurecolumns])\n",
    "    staticfeatureset = filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns].iloc[0]\n",
    "    return staticfeatureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDFMin(column):\n",
    "    row1 = df.agg({column: \"min\"}).collect()[0]\n",
    "    return(row1[\"min(\" + column + \")\"])\n",
    "\n",
    "def getDFMax(column):\n",
    "    row1 = df.agg({column: \"max\"}).collect()[0]\n",
    "    return(row1[\"max(\" + column + \")\"])\n",
    "    \n",
    "dfminStart = getDFMin(\"CSTARTTIME\")\n",
    "dfmaxStart = getDFMax(\"CSTARTTIME\")\n",
    "\n",
    "#dfminEnd = getDFMin(\"CENDTIME\")\n",
    "#dfmaxEnd = getDFMax(\"CENDTIME\")\n",
    "\n",
    "dfrowCount     = df.count()\n",
    "dfcolumnCount  = len(df.columns)\n",
    "\n",
    "#numbermessages = df.select('CGLOBALMESSAGEID').dropDuplicates().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfminStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMinMaxDate(prefix,dfminTimestamp,dfmaxTimestamp):\n",
    "    print(prefix + ': min:',common.date(float(dfminTimestamp) / 1e3), 'max:', common.date(float(dfmaxTimestamp) / 1e3))\n",
    "    \n",
    "printMinMaxDate('CSTARTTIME', dfminStart,dfmaxStart)    \n",
    "#printMinMaxDate('CENDTIME', dfminEnd,dfmaxEnd) \n",
    "print('rows#:',dfrowCount,'columns#:',dfcolumnCount)\n",
    "#print('messages#:',numbermessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B Step2\n",
    "#pfall = df.limit(100000).toPandas() \n",
    "pfall = df.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "# long timestamp\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x) / 1e3, tz=de)\n",
    "\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: date(x).date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: date(x).date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: date(x).date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: date(x).time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: date(x).time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt.datetime.fromtimestamp(float(pfall.iloc[0]['CSTARTTIME']) / 1e3)\n",
    "\n",
    "#date(pfall.iloc[0]['CSTARTTIME'])\n",
    "#date(1579064281477).date().year\n",
    "del(pfall['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astype(pfall,selected,newtype):\n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(newtype)\n",
    "        \n",
    "selected =  ['CSENDERENDPOINTID']       \n",
    "astype(pfall,selected,str)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B\n",
    "selected = [ 'CSTATUS', 'CSERVICE',\\\n",
    "        'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "        'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID']\n",
    "\n",
    "astype(pfall,selected,str) \n",
    "encoder.encode(pfall,selected)\n",
    "astype(pfall,['CSTARTTIME','CENDTIME','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CINBOUNDSIZE'] ,int) \n",
    "#del(pfall['CSLABILLINGMONTH'])\n",
    "pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "pfall = pfall.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountDF(pf,column,hashes):\n",
    "    dft = pd.DataFrame(columns=[column, 'count'])\n",
    "    i=0\n",
    "    for hash in hashes:\n",
    "        pfall=pf[pf[column] == hash]\n",
    "        num=len(pfall)\n",
    "        if num > 1:\n",
    "            dft.loc[i] = [hash] + [num]\n",
    "            i=i+1\n",
    "    return dft.sort_values('count')\n",
    "\n",
    "def usedcolumns(tb,row):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[row][column] == None :\n",
    "            col.append(column)\n",
    "    return col\n",
    "\n",
    "def diffcolumns(tb):\n",
    "    col = []\n",
    "    for column in tb.columns:\n",
    "        if tb.iloc[0][column] != tb.iloc[1][column] :\n",
    "            col.append(column)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ac = pd.unique(pfall['CGLOBALMESSAGEID'])\n",
    "tt = getCountDF(pfall,'CGLOBALMESSAGEID',ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tt)\n",
    "#tt[tt['count'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.unique(pfall['CGLOBALMESSAGEID'])),len(pfall)\n",
    "#tt[tt['count'] > 1]\n",
    "#tt\n",
    "#pfall[pfall['CGLOBALMESSAGEID'] == ac[0]]\n",
    "#usedcolumns(tb,0)\n",
    "#usedcolumns(tb,1)\n",
    "#tb = pfall[pfall['CGLOBALMESSAGEID'] == tt.iloc[300]['CGLOBALMESSAGEID']]\n",
    "\n",
    "  \n",
    "def printtt():    \n",
    "    for index, row in tt.iterrows():\n",
    "        print(row['CGLOBALMESSAGEID'])\n",
    "        tb = pfall[pfall['CGLOBALMESSAGEID'] == row['CGLOBALMESSAGEID']]\n",
    "        print(diffcolumns(tb))\n",
    "    \n",
    "tb = pfall[pfall['CGLOBALMESSAGEID'] == tt.iloc[1]['CGLOBALMESSAGEID']]\n",
    "tbindex = tb.index[1]\n",
    "pfall.loc[tbindex]['CGLOBALMESSAGEID']\n",
    "if pfall.loc[tbindex]['CSLADELIVERYTIME'] < 0 :\n",
    "        print(\"remove \" + pfall.loc[tbindex]['CGLOBALMESSAGEID'])\n",
    "        pfall = pfall.drop(tbindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printtt():    \n",
    "    col = []\n",
    "    for index, row in tt.iterrows():\n",
    "        tb = pfall[pfall['CGLOBALMESSAGEID'] == row['CGLOBALMESSAGEID']]\n",
    "        for bindex, brow in tb.iterrows():\n",
    "            if pfall.loc[bindex]['CSLADELIVERYTIME'] < 0 :\n",
    "                #print (str(index) + ' ' + str(bindex))\n",
    "                col.append(bindex)\n",
    "    return col\n",
    "\n",
    "col = printtt()   \n",
    "pfall = pfall.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall[pfall['CGLOBALMESSAGEID'] == tt.iloc[1]['CGLOBALMESSAGEID']]\n",
    "#tt\n",
    "pfall.head()\n",
    "#pfall.dtypes\n",
    "#\n",
    "#pfall[(pfall['outcome'] == 0) & (pfall['CSTATUS'] == 'PENDING')]\n",
    "#pfall[(pfall['outcome'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall[pfall['CGLOBALMESSAGEID'] == tt.iloc[0]['CGLOBALMESSAGEID']].iloc[0]['CGLOBALMESSAGEID'], \\\n",
    "pfall.loc[10468]['CGLOBALMESSAGEID']\n",
    "\n",
    "\n",
    "#pfall.loc[tt.loc[1]['index']]['CGLOBALMESSAGEID'], tt.loc[1]['CGLOBALMESSAGEID']\n",
    "#pfall.loc[tt.loc[1]['index']], tt.loc[1]\n",
    "#len(pfall)\n",
    "#len(pfall['CGLOBALMESSAGEID'].drop_duplicates()), len(pfall)\n",
    "#tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.encode(pfall,selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [  'CSENDERENDPOINTID', 'CRECEIVERENDPOINTID']\n",
    "\n",
    "astype(pfall,selected,str) \n",
    "encoder.encode(pfall,selected)\n",
    "#astype(pfall,['CSTARTTIME','CENDTIME','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CINBOUNDSIZE'] ,int) \n",
    "#del(pfall['CSLABILLINGMONTH'])\n",
    "pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "pfall = pfall.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime and add column date\n",
    "#import calendar\n",
    "##import pytz\n",
    "#de = pytz.timezone('Europe/Berlin')\n",
    "adddatecolumns(pfall,pfall,'CSTARTTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = pfall.assign(outcome=(~( ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'InvoicePortal')) | ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'IDS')) | (pfall['CSTATUS'] == 'SUCCESS') | (pfall['CSTATUS'] == 'SUCCESS_DOWNLOADED') | (pfall['CSTATUS'] == 'SUCCESS_POLLQUEUE'))).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(pfall,['CSTARTTIME'] ,int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall.to_parquet('/tmp/msgsenders_0702.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_parquet('/tmp/msgsenders.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b\n",
    "    #return a.join([\"-\",b]) \n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ymd(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a,b,c\n",
    "\n",
    "def get_ymd_string(a,b,c) :\n",
    "    return a + \"-\" + b + \"-\" + c \n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    return a + \"-\" + b + \"-\" + c + \"-\" + d\n",
    "\n",
    "def createData_ym(pfall,month) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month'])['year'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData(pfall,month) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day','hour'])['year'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int)\n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ymd(pfall,month) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day'])['year'].count()    \n",
    "    a,b,c = get_ymd(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymd_string(a,b,c)\n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "\n",
    "def createBarplot(md,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=75 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createData(pfall,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall.head()\n",
    "pfall[pfall['CGLOBALMESSAGEID'] == '2af51910-9e8b-11ea-aad1-6c31ac1e100c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0)\n",
    "ax=createBarplot(md,24,12,3.0,title=\"number messages sent by all endpoints\")\n",
    "#label(ax,1000,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ymd(pfall,6)\n",
    "ax=createBarplot(md,24,12,3.0,title=\"number messages sent by all endpoints\")\n",
    "label(ax,1000,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = createData(pfall,7)\n",
    "piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "#titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "#titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "titlestring =\"number messages 2020-5\"\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "createHeatmap(piv, titlestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = pd.read_parquet('/tmp/msgsenders_0702.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall = pfall.sort_values('CSTARTTIME').reset_index() \n",
    "pfall\n",
    "#len (pfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection with LSTM Autoencoders (selected sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "TOP=500000\n",
    "TOP=5000\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['CSTARTTIME'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > TOP].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']\n",
    "topsender['outcome'] = topsender['outcome'].astype(int)\n",
    "topsender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['CSTARTTIME'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[9]['CGLOBALMESSAGEID'],len(pfall[pfall['CSENDERENDPOINTID'] == 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = pd.unique(pfall['CSENDERENDPOINTID']) \n",
    "len(senders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall[pfall['CSENDERENDPOINTID'] == sender].sort_values('CSTARTTIME').reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sender = 2191\n",
    "sender = 1512\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID'] == sender].sort_values('CSTARTTIME').reset_index() \n",
    "\n",
    "def get_datestr(row):\n",
    "    return str(row.day) + \".\" + str(row.month) + \".\" + str(row.year) \n",
    "\n",
    "\n",
    "pfall1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall.columns\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall1 = pfall.sort_values(['year','month','day','hour']).reset_index()\n",
    "del pfall1['index']\n",
    "del pfall1['CGLOBALMESSAGEID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME = 'close'\n",
    "\n",
    "#TIME_STEPS = 1\n",
    "#TIME_STEPS = 24\n",
    "TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df1 = createDataframe(pfall1)\n",
    "train, test = getTrainAndTest(df1,0.85)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 0.8)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = 0\n",
    "\n",
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)\n",
    "\n",
    "titlestring =\"endpoint (sending) : \" + str(sender) + \" ( trained: \" +  train.index[0] + \" -- \" + train.index[len(train.index)-1] + \" )\"\n",
    "\n",
    "plt.title(titlestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pfall[pfall['CSENDERENDPOINTID'] == 1774].reset_index().to_csv('/tmp/1774.csv', sep=';')\n",
    "#pfall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pfall['CGLOBALMESSAGEID'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pfall['CGLOBALMESSAGEID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall[pfall['CGLOBALMESSAGEID'] == '5ace4183-9e8f-11ea-b980-22fcac1e100b']\n",
    "columns = df.limit(1).toPandas().columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.limit(1).toPandas().columns\n",
    "withoutColumns = ['timestamp', 'value','schemaid', 'hashvalue']\n",
    "columns = columns[~columns.isin(withoutColumns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "schemas = dfcolumnSeries('schemaid').toPandas()['schemaid']\n",
    "igroups = dfcolumnSeries('igroup').toPandas()['igroup']\n",
    "\n",
    "schemaidCount  = dfcolumnCount('schemaid')\n",
    "hashvalueCount = dfcolumnCount('hashvalue')\n",
    "igroupCount = dfcolumnCount('igroup')\n",
    "print('schemaidCount: ',schemaidCount,'hashvalueCount: ',hashvalueCount,'igroupCount: ',igroupCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = getIgroupHashes(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hash = hashes[10]\n",
    "staticfeatureset = getStaticFeatures(_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedhashes = pd.unique(pfall[eval1(pfall)]['hashvalue'])\n",
    "relatedhashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForRelatedHashes(relatedhashes):\n",
    "    data2 = pd.DataFrame()\n",
    "    start = True\n",
    "    for hash_ in relatedhashes:\n",
    "        c1 =  getDFvalueWithHash(int(hash_))\n",
    "        if start:\n",
    "            data2['date'] = c1['timestamp']\n",
    "            start = False\n",
    "        data2[str(hash_)] =  c1['value']\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getDataForRelatedHashes(relatedhashes)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.lineplot(data=data2.iloc[:, :2])\n",
    "sns.lineplot(data=data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows = pfall[pfall['igroup'] == 6]\n",
    "\n",
    "len(filteredrows)\n",
    "hashes = pd.unique(filteredrows['hashvalue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "getMissingSchemaids(pfall,hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame()\n",
    "#pfm = getDFvalueWithHash(int(relatedhashes[0]))\n",
    "#\n",
    "data2['date'] = dt['timestamp'].astype(str)\n",
    "data2['value'] = dt['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashes_d = dfcolumnSeries('hashvalue').toPandas()['hashvalue']\n",
    "hashes_d = df.select(['hashvalue','schemaid']).dropDuplicates().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(hashes_d['hashvalue']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pd.unique(hashes_d))\n",
    "\n",
    "\n",
    "dt = getDFvalueWithHash(int(hashes_d['schemaid'][0]),int(hashes_d['hashvalue'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if it is unique\n",
    "len(dt['timestamp']), len(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(pfall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hashes))\n",
    "len(pd.unique(pfall['hashvalue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getInfoForSchemas(filteredrows)\n",
    "#len(pd.unique(filteredrows['__name__']))\n",
    "#getEncoder('__name__').inverse_transform(pd.unique(filteredrows['__name__']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall[(pfall['schemaid'] == 1629035211) & (pfall['timestamp'] == 1587819506)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getEncoder('igroup').inverse_transform(igroups)\n",
    "for igroup in igroups:\n",
    "    filteredrows = pfall[pfall['igroup'] == igroup]\n",
    "    count = len(pd.unique(filteredrows['inode']))\n",
    "    if count > 1:\n",
    "        print(getEncoder('igroup').inverse_transform([igroup]),igroup,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hash_ in hashes:\n",
    "    metric = filteredrows[filteredrows['hashvalue']==hash_]\n",
    "    count = len(pd.unique(metric['value']))\n",
    "    if count > 3:\n",
    "        print(hash_,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue with hash\n",
    "#_hash=-2018833881\n",
    "_hash=-283862276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!! missing schemas\n",
    "getMissingSchemaids(pfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[filteredrows['hashvalue'] == _hash]\n",
    "filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns].iloc[0]['__name__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#['schemaid','igroup']\n",
    "\n",
    "#filteredrows[featurecolumns].head()\n",
    "#getVariableUniqueColums(filteredrows[featurecolumns])\n",
    "\n",
    "# there are still too many columns for _hash\n",
    "#len(getVariableUniqueColums(filteredrows[featurecolumns]))\n",
    "#len(featurecolumns)\n",
    "\n",
    "\n",
    "# there are no variable columns left\n",
    "#getVariableUniqueColums(filteredrows[filteredrows['hashvalue'] == _hash][featurecolumns])\n",
    "\n",
    "# static feature values for metric _hash\n",
    "\n",
    "#staticfeatureset[featurecolumns[1]]\n",
    "#staticfeatureset\n",
    "\n",
    "\n",
    "#featurecolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currenthash     = int(relatedhashes[1])\n",
    "currentfeatures = filteredrows[filteredrows['hashvalue'] == currenthash].iloc[0]\n",
    "currentfeatures['igroup'],currentfeatures['inode'],currentfeatures['hashvalue'],getEncoder('__name__').inverse_transform([currentfeatures['__name__']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 = data2.sort_values('date').reset_index()\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = createLineplot(data2,16,10,1.4,title=\"\",skip=500)\n",
    "label(ax,10,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[featurecolumns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out variable columns and type of value\n",
    "#getVariableUniqueColums(pfm)\n",
    "#selected = ['timestamp', 'value','year','month', 'day', 'hour', 'minute']\n",
    "\n",
    "#which inode is used by metri hashvalue\n",
    "used_inode = filteredrows[filteredrows['hashvalue'] == _hash]['inode'].iloc[0]\n",
    "\n",
    "# other inodes for the igroup\n",
    "inodes_igroup = pd.unique(filteredrows[filteredrows['igroup'] == 6]['inode'])\n",
    "otherinodes = inodes_igroup[inodes_igroup!=used_inode]\n",
    "\n",
    "#\n",
    "pd.unique(filteredrows[filteredrows['igroup'] == 6]['__name__'])\n",
    "pd.unique(filteredrows[(filteredrows['igroup'] == 6) & (filteredrows['__name__'] == 309)]['logical_system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredrows[filteredrows['inode'] == inodes_igroup[0]]['hashvalue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "metric = filteredrows[filteredrows['hashvalue']==_hash]\n",
    "#metric = filteredrows[filteredrows['hashvalue']==hashes[0]]\n",
    "#usedcolumns(metric)\n",
    "\n",
    "keep = getVariableUniqueColums(metric)\n",
    "#keepcolumns(dataall,keep)\n",
    "#keep\n",
    "len(metric),len(pd.unique(metric['value'])), pd.unique(metric['value'])\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall1 = pfall[pfall['schemaid'] == schemas[0]]\n",
    "mdcountsall = pfall1.groupby(['igroup','hashvalue'])\n",
    "#['igroup'].count() \n",
    "#data2 = pd.DataFrame()\n",
    "mdcountsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getEncoder('__name__').inverse_transform([pfall1.iloc[0]['__name__']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "os.environ['https_proxy'] = \"https://172.30.12.56:3128\"   \n",
    "#!conda install -y -c conda-forge fastparquet\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileSystem.get(sparkSession.hadoopConfiguration).listFiles('hdfs://172.30.17.145:8020/sla_sql_data/1580137124017', true)\n",
    "\n",
    "#path.getFileSystem(sparkSession.hadoopConfiguration).listFiles(hdfs://172.30.17.145:8020/sla_sql_data/, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'hdfs://172.30.17.145:8020/sla_sql_data/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esutil as eu\n",
    "eu.hdfs.ls(hdfs_url='hdfs://172.30.17.145:8020/sla_sql_data/1580137124017', recurse=False, full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://172.30.12.56:3128\" \n",
    "os.environ['https_proxy'] = \"https://172.30.12.56:3128\"   \n",
    "!pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install esutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "client = Config().get_client('dev')\n",
    "client.list('/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "client = Config().get_client('dev')\n",
    "mypath = 'hdfs://172.30.17.145:8020/sla_sql_data/1580137124017'\n",
    "files = client.list(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jovyan/.hdfscli.cfg\", \"w\") as file:\n",
    "    file.write(\"[global]\\n\") \n",
    "    file.write(\"default.alias = dev\\n\") \n",
    "    file.write(\"[dev.alias]\\n\") \n",
    "    file.write(\"url = https://172.30.17.145:8020\\n\") \n",
    "    file.write(\"user = admin\\n\") "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
