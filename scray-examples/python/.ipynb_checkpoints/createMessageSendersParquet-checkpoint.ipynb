{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base for Tools to encode messages (current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfBasics\n",
    "import common\n",
    "import encoder\n",
    "import pfAdapt\n",
    "#import charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "       'CSLABILLINGMONTH', 'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "       'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "       'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "# withot 'CSLABILLINGMONTH'\n",
    "def get_columns_2():\n",
    "    columns = ['CGLOBALMESSAGEID', 'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE',\\\n",
    "            'CSENDERPROTOCOL', 'CSENDERENDPOINTID',\\\n",
    "           'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT',\\\n",
    "           'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "    return columns\n",
    "\n",
    "columns = ['CGLOBALMESSAGEID',  'CSTARTTIME', 'CENDTIME', 'CSTATUS', 'CSERVICE', 'CSENDERENDPOINTID', 'CSENDERPROTOCOL', 'CINBOUNDSIZE', 'CRECEIVERPROTOCOL', 'CRECEIVERENDPOINTID', 'CSLATAT', 'CMESSAGETAT2', 'CSLADELIVERYTIME']\n",
    "     \n",
    "\n",
    "#columns = get_columns_2()\n",
    "#to count messages sent\n",
    "#columns = [ 'CSTARTTIME', 'CSENDERENDPOINTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/16 08:46:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/16 08:46:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "sparkSession = dfBasics.getSparkSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*').select(columns).dropDuplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /tmp/sla.parquet\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/sla.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet(\"/tmp/sla.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix null endpoints\n",
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"CSENDERENDPOINTID\", when(df.CSENDERENDPOINTID.isNull(), \"None\").otherwise(df.CSENDERENDPOINTID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /tmp/sla.parquet\n",
    "\n",
    "df2.write.mode(\"overwrite\").parquet(\"/tmp/sla.parquet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create file with current sending endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*').select(['CSENDERENDPOINTID']).dropDuplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "senders = df.toPandas() \n",
    "senders.to_parquet('/tmp/senders' + '.parquet', engine='pyarrow', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = df2.select([ 'CSENDERENDPOINTID']).dropDuplicates().toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders.to_parquet('/tmp/senders' + '.parquet', engine='pyarrow', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = sparkSession.read.parquet(\"/tmp/senders.parquet\")\n",
    "senders = list(senders.toPandas()['CSENDERENDPOINTID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create column encoders (pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import encoder\n",
    "\n",
    "def createEncoders(dataall,columns):\n",
    "    for column in columns:\n",
    "        le = encoder.TolerantLabelEncoder(ignore_unknown=True)\n",
    "        #le.fit([1, 2, 2, 6])\n",
    "        le.fit(dataall[column])\n",
    "        encoder.LabelEncoder()\n",
    "        #print(le.classes_)\n",
    "        np.save('/home/jovyan/work/cls/jupyter/npy/' + column + '.npy', le.classes_)\n",
    "        \n",
    "#!ls /home/jovyan/work/cls/jupyter/npy\n",
    "#createEncoders(senders,['CSENDERENDPOINTID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.limit(1).toPandas().columns)\n",
    "columns.remove('CGLOBALMESSAGEID')\n",
    "columns.remove('CSLATAT')\n",
    "columns.remove('CMESSAGETAT2') \n",
    "columns.remove('CSLADELIVERYTIME')\n",
    "columns.remove('CINBOUNDSIZE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    print (column)\n",
    "    pfall=df.select([column]).dropDuplicates().toPandas()\n",
    "    createEncoders(pfall,[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def listdirectory(path=None,filter='.'):\n",
    "    return [x for x in listdir(path) if not x.startswith(filter)]    \n",
    "\n",
    "_files = listdirectory(path='/tmp/enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.parquet(\"/tmp/sla.parquet\")\n",
    "#senders = pd.read_parquet('/tmp/senders' + '.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def transform(value):\n",
    "    return int( _encoder.transform([value])[0])\n",
    "    \n",
    "udf_transform = udf(lambda z: transform(z), StringType())\n",
    "\n",
    "#df2.withColumn(\"CSENDERENDPOINTID\", str( _encoder.transform([df2.CSENDERENDPOINTID])[0])) \n",
    "#df2 = df2.withColumn(\"CSENDERENDPOINTID\", udf_transform(df2.CSENDERENDPOINTID)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senders = sparkSession.read.parquet(\"/tmp/senders.parquet\")\n",
    "senders = list(senders.toPandas()['CSENDERENDPOINTID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.limit(1).toPandas().columns)\n",
    "columns.remove('CGLOBALMESSAGEID')\n",
    "columns.remove('CSLATAT')\n",
    "columns.remove('CMESSAGETAT2') \n",
    "columns.remove('CSLADELIVERYTIME')\n",
    "columns.remove('CINBOUNDSIZE')\n",
    "columns.remove('CSTARTTIME')\n",
    "columns.remove('CENDTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# long timestamp\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x) / 1e3, tz=de)\n",
    "\n",
    "udf_add_year = udf(lambda z: date(z).date().year, IntegerType())\n",
    "udf_add_month = udf(lambda z: date(z).date().month, IntegerType())\n",
    "udf_add_day = udf(lambda z: date(z).date().day, IntegerType())\n",
    "udf_add_hour = udf(lambda z: date(z).time().hour, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import encoder\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def encode_columns_spark(dataframe=None,columns=None):\n",
    "    for column in columns:\n",
    "        global _encoder\n",
    "        #print (column)\n",
    "        _encoder = encoder.TolerantLabelEncoder(ignore_unknown=True)\n",
    "        _encoder.classes_ = np.load('/home/jovyan/work/cls/jupyter/npy/' + column + '.npy')\n",
    "        #dataall[column] = _encoder.transform(dataall[column]) \n",
    "        udf_transform = udf(lambda z: transform(z), StringType())\n",
    "        dataframe=dataframe.withColumn(column, udf_transform(col(column)).cast(\"Integer\"))\n",
    "        #df3.head()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_spark_columns(dataframe=None,columns=[],type=\"int\" ):\n",
    "    for column in columns:\n",
    "        dataframe = dataframe.withColumn(column, col(column).cast(type))\n",
    "    return dataframe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sender=None, dataframe=None):\n",
    "    df3 = dataframe.where(f.col(\"CSENDERENDPOINTID\").isin([sender]))\n",
    "    df3 = encode_columns_spark(dataframe=df3,columns=columns)\n",
    "    df3 = df3.withColumn(\"year\", udf_add_year(df3.CSTARTTIME)).withColumn(\"month\", udf_add_month(df3.CSTARTTIME)).withColumn(\"day\", udf_add_day(df3.CSTARTTIME)).withColumn(\"hour\", udf_add_hour(df3.CSTARTTIME)) \n",
    "    df3=cast_spark_columns(dataframe=df3, columns=['CSTARTTIME', 'CENDTIME','CINBOUNDSIZE','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME'], type='long')\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "#np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "#sender = senders[0]\n",
    "for sender in senders:\n",
    "    df4 = process(sender=sender,dataframe=df)\n",
    "    df4.write.mode(\"overwrite\").parquet(\"/tmp/enc/sla_enc_\" + sender + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /tmp/enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /tmp/sla_enc_772e6440-e973-11e8-be62-528eac1b495c.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df.limit(1).toPandas().columns)\n",
    "\n",
    "#pfall=df3.toPandas()\n",
    "#pfall.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4 =  df3.withColumn(\"CSENDERENDPOINTID\", udf_transform(df3.CSENDERENDPOINTID).cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4.show(10)\n",
    "!date\n",
    "df4.write.mode(\"overwrite\").parquet(\"/tmp/sla_enc.parquet\")\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "#np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall=df3.toPandas()\n",
    "pfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = df3.withColumn(\"CSTARTTIME\", col(\"CSTARTTIME\").cast(\"decimal(38,0)\"))\n",
    "df3 = df3.withColumn(\"CSTARTTIME\", col(\"CSTARTTIME\").cast(\"long\"))\n",
    "pfall=df3.toPandas()\n",
    "pfall.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.limit(1).toPandas().columns).remove('CGLOBALMESSAGEID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_encoder.transform(['None'])\n",
    "_encoder.transform(['0014de30-f8ec-11eb-9933-02daac1e124d'])\n",
    "#!ls /home/jovyan/work/jupyter/npy/    \n",
    "#!pwd\n",
    "_encoder.inverse_transform(4866)\n",
    "#len(_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 =  df2.limit(50000000).withColumn(\"CSENDERENDPOINTID\", udf_transform(df2.CSENDERENDPOINTID).cast(\"Integer\"))\n",
    "df3 =  df2.withColumn(\"CSENDERENDPOINTID\", udf_transform(df2.CSENDERENDPOINTID).cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date\n",
    "df3.head()\n",
    "#df3.show(10)\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df3 = df3.withColumn(\"CSENDERENDPOINTID\", df3[\"CSENDERENDPOINTID\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "df3.head(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/sla_enc.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.mode(\"overwrite\").parquet(\"/tmp/sla_enc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = df3.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "\n",
    "# long timestamp\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x) / 1e3, tz=de)\n",
    "\n",
    "\n",
    "def adddatecolumns(data,pf,column) :\n",
    "    data['year'] = pf[column].apply(lambda x: date(x).date().year)\n",
    "    data['month'] = pf[column].apply(lambda x: date(x).date().month)\n",
    "    data['day'] = pf[column].apply(lambda x: date(x).date().day)\n",
    "    data['hour'] = pf[column].apply(lambda x: date(x).time().hour)\n",
    "    data['minute'] = pf[column].apply(lambda x: date(x).time().minute)\n",
    "    #data['second'] = pf[column].apply(lambda x: x.time().second)\n",
    "    #data['microsecond'] = pf[column].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astype(pfall,selected,newtype):\n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(newtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [  'CSENDERENDPOINTID']\n",
    "\n",
    "astype(pfall,selected,str) \n",
    "encoder.encode(pfall,selected)\n",
    "#astype(pfall,['CSTARTTIME','CENDTIME','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME','CINBOUNDSIZE'] ,int) \n",
    "#del(pfall['CSLABILLINGMONTH'])\n",
    "#pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "pfall = pfall.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime and add column date\n",
    "#import calendar\n",
    "##import pytz\n",
    "#de = pytz.timezone('Europe/Berlin')\n",
    "adddatecolumns(pfall,pfall,'CSTARTTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(pfall,['CSTARTTIME'] ,int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time; ts = str(time.time()) \n",
    "pfall.to_parquet('/tmp/msgsenders_' + ts + '.parquet', engine='fastparquet', compression='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv /tmp/msgsenders_date.parquet /tmp/msgsenders_200716.parquet\n",
    "import time; ts = time.time() \n",
    "print(ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
