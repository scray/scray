{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f1472-359b-4322-a2df-ada096766583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import encoder\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a3fbb-6e46-4f4a-80ca-042291decaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import pytz\n",
    "de = pytz.timezone('Europe/Berlin')\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# long timestamp\n",
    "def date(x):\n",
    "    return  dt.datetime.fromtimestamp(float(x) / 1e3, tz=de)\n",
    "\n",
    "udf_add_year = udf(lambda z: date(z).date().year, IntegerType())\n",
    "udf_add_month = udf(lambda z: date(z).date().month, IntegerType())\n",
    "udf_add_day = udf(lambda z: date(z).date().day, IntegerType())\n",
    "udf_add_hour = udf(lambda z: date(z).time().hour, IntegerType())\n",
    "udf_add_minute = udf(lambda z: date(z).time().minute, IntegerType())\n",
    "udf_add_minute = udf(lambda z: date(z).time().minute, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "324df34f-ddc9-4e67-81d0-fefb04ff4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def transform(value):\n",
    "    return int( _encoder.transform([value])[0])\n",
    "    \n",
    "udf_transform = udf(lambda z: transform(z), StringType())\n",
    "\n",
    "#df2.withColumn(\"CSENDERENDPOINTID\", str( _encoder.transform([df2.CSENDERENDPOINTID])[0])) \n",
    "#df2 = df2.withColumn(\"CSENDERENDPOINTID\", udf_transform(df2.CSENDERENDPOINTID)) \n",
    "\n",
    "def get_columns(df):\n",
    "    columns = list(df.limit(1).toPandas().columns)\n",
    "    columns.remove('CGLOBALMESSAGEID')\n",
    "    columns.remove('CSLATAT')\n",
    "    columns.remove('CMESSAGETAT2') \n",
    "    columns.remove('CSLADELIVERYTIME')\n",
    "    columns.remove('CINBOUNDSIZE')\n",
    "    columns.remove('CSTARTTIME')\n",
    "    columns.remove('CENDTIME')\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f06dac9-d059-4128-aa93-440acafed7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_columns_spark(dataframe=None,columns=None, encoders=None):\n",
    "    for column in columns:\n",
    "        global _encoder\n",
    "        _encoder = encoders[column]\n",
    "        udf_transform = udf(lambda z: transform(z), StringType())\n",
    "        dataframe=dataframe.withColumn(column, udf_transform(col(column)).cast(\"Integer\"))\n",
    "    return dataframe\n",
    "\n",
    "def cast_spark_columns(dataframe=None,columns=[],type=\"int\" ):\n",
    "    for column in columns:\n",
    "        dataframe = dataframe.withColumn(column, col(column).cast(type))\n",
    "    return dataframe    \n",
    "\n",
    "def process(sender=None, dataframe=None):\n",
    "    df3 = dataframe.where(f.col(\"CSENDERENDPOINTID\").isin([sender]))\n",
    "    df3 = encode_columns_spark(dataframe=df3,columns=columns)\n",
    "    df3 = df3.withColumn(\"year\", udf_add_year(df3.CSTARTTIME)).withColumn(\"month\", udf_add_month(df3.CSTARTTIME)).withColumn(\"day\", udf_add_day(df3.CSTARTTIME)).withColumn(\"hour\", udf_add_hour(df3.CSTARTTIME)).withColumn(\"minute\", udf_add_minute(df3.CSTARTTIME)) \n",
    "    df3=cast_spark_columns(dataframe=df3, columns=['CSTARTTIME', 'CENDTIME','CINBOUNDSIZE','CSLATAT','CMESSAGETAT2','CSLADELIVERYTIME'], type='long')\n",
    "    return df3\n",
    "\n",
    "\"\"\"\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "#np.load = np_load_old\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbeb0c-4c8e-4278-a7b3-c9c5bb17d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "#import pyspark.sql.functions as f\n",
    "#ender = senders[0]\n",
    "#df4 = process(sender=sender,dataframe=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
