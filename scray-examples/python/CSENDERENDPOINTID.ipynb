{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "def adddatecolumns(data,pf) :\n",
    "    data['year'] = pf['CSTARTTIME'].apply(lambda x: x.date().year)\n",
    "    data['month'] = pf['CSTARTTIME'].apply(lambda x: x.date().month)\n",
    "    data['day'] = pf['CSTARTTIME'].apply(lambda x: x.date().day)\n",
    "    data['hour'] = pf['CSTARTTIME'].apply(lambda x: x.time().hour)\n",
    "    data['minute'] = pf['CSTARTTIME'].apply(lambda x: x.time().minute)\n",
    "    data['second'] = pf['CSTARTTIME'].apply(lambda x: x.time().second)\n",
    "    data['microsecond'] = pf['CSTARTTIME'].apply(lambda x: x.time().microsecond)\n",
    "\n",
    "def converttimestampcolumnn(pf,tsc) :\n",
    "    pf[tsc] = pf[tsc].apply(lambda x: dt.datetime.fromtimestamp(float(x) / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "#vis functions\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "            \n",
    "def abc(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a + \"-\" + b + \"-\" + c \n",
    "\n",
    "def abcd(mdcountsall) :\n",
    "    return abc(mdcountsall) + \"-\" + mdcountsall.index.get_level_values(3).astype(str)\n",
    "\n",
    "def ymdh(mdcountsall) :\n",
    "    return abcd(mdcountsall)\n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    return a + \"-\" + b + \"-\" + c + \"-\" + d\n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mdcountsall=pfall[(pfall['month'] == 2) & (pfall['day'] > 16)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall[(pfall['month'] == 2)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "#mdcountsall=pfall.groupby(['year','month','day'])['outcome'].count()\n",
    "\n",
    "def createData(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['month'] == month) & (pfall['outcome'] == outcome)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month','day','hour'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month','day','hour'])['outcome'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ym(pfall,month,outcome) :\n",
    "    if outcome < 2 :\n",
    "        mdcountsall = pfall[(pfall['outcome'] == outcome)].groupby(['year','month'])['outcome'].count()\n",
    "    else :\n",
    "        if (month > 0) & (month < 13) :\n",
    "            mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['outcome'].count()\n",
    "        else :\n",
    "            mdcountsall = pfall.groupby(['year','month'])['outcome'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] =  mdcountsall.reset_index()['outcome'].astype(int) \n",
    "\n",
    "    #for pivot table\n",
    "    #data2['hours'] =  d.astype(int) \n",
    "    #data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def createBarplot(md,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "# kernel density estimate (KDE) \n",
    "def createKDE(data2,fx,fy,fontscale,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    # Histogram \n",
    "    #ax = sns.distplot(a=data2['outcome'], kde=False)\n",
    "    ax = sns.kdeplot(data=data2['outcome'], shade=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Anomaly Detection with LSTM Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "OUTCOME = 'close'\n",
    "\n",
    "TIME_STEPS = 24\n",
    "#TIME_STEPS = 30\n",
    "#TIME_STEPS = 720\n",
    "#TIME_STEPS = 168\n",
    "#TIME_STEPS = 336\n",
    "\n",
    "# setup data (current)\n",
    "def createDataframe(pfall) :\n",
    "    data3 = createData(pfall,0,2)\n",
    "    df = pd.DataFrame()\n",
    "    df[OUTCOME] = data3['outcome']\n",
    "    df.set_index(data3['date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def getTrainAndTest(df,TRAIN_SIZE) :\n",
    "    train_size = int(len(df) * TRAIN_SIZE)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "    print(\"train.shape: \",train.shape, \"test.shape: \", test.shape)\n",
    "    return train, test\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def initmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(\n",
    "        units=64, \n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "    model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def testScoreDF(model, THRESHOLD) : \n",
    "    X_test_pred = model.predict(X_test)\n",
    "    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "    test_score_df['loss'] = test_mae_loss\n",
    "    test_score_df['threshold'] = THRESHOLD\n",
    "    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "    test_score_df[OUTCOME] = test[TIME_STEPS:][OUTCOME]\n",
    "    return test_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "#sc = SparkContext()\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparkSession = SparkSession.builder.config(\"spark.executor.memory\", \"8g\").config(\"spark.driver.memory\", \"8g\").config(\"spark.driver.maxResultSize\", \"0\").appName(\"example-pyspark-read-and-write\").getOrCreate()\n",
    "\n",
    "\n",
    "df = sparkSession.read.parquet('hdfs://172.30.17.145:8020/sla_sql_data/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cat_features = ['CSERVICE','CRECEIVERPROTOCOL','CSENDERPROTOCOL','CSENDERENDPOINTID','CRECEIVERENDPOINTID','CSTATUS']\n",
    "\n",
    "def encodeall(pfall,cat_features):\n",
    "    #Prepping categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    # Apply the label encoder to each column\n",
    "    encodedpfall = pfall[cat_features].apply(encoder.fit_transform)\n",
    "    return encodedpfall\n",
    "  \n",
    "def getpfbasic(pfall) :   \n",
    "    #pfall = df2.limit(5000000).toPandas()    \n",
    "    #print(pfall.loc[pfall['CSTARTTIME'].idxmax()]['CSTARTTIME'],len(pfall.index))\n",
    "    pfall = pfall.assign(outcome=(~( ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'InvoicePortal')) | ((pfall['CSTATUS'] == 'PENDING') & (pfall['CSERVICE'] == 'IDS')) | (pfall['CSTATUS'] == 'SUCCESS') | (pfall['CSTATUS'] == 'SUCCESS_DOWNLOADED') | (pfall['CSTATUS'] == 'SUCCESS_POLLQUEUE'))).astype(int))\n",
    "    converttimestampcolumnn(pfall,'CSTARTTIME')\n",
    "    pfall['CGLOBALMESSAGEID'] = pfall['CGLOBALMESSAGEID'].apply(hash)\n",
    "    #pfall['CSENDERENDPOINTID'] = pfall['CSENDERENDPOINTID'].astype(str)   \n",
    "    #pfall['CRECEIVERENDPOINTID'] = pfall['CRECEIVERENDPOINTID'].astype(str)\n",
    "    pfall['CMESSAGETAT2'] = pfall['CMESSAGETAT2'].astype(int)\n",
    "    pfall['CSLATAT'] = pfall['CSLATAT'].astype(int)\n",
    "    pfall['CINBOUNDSIZE'] = pfall['CINBOUNDSIZE'].astype(int)\n",
    "    return pfall\n",
    "\n",
    "def getpf(df2) :    \n",
    "    pfall = getpfbasic(df2)\n",
    "    encodedpfall = encodeall(pfall,cat_features)\n",
    "    dataall = pfall[['CGLOBALMESSAGEID','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE', 'outcome']].join(encodedpfall)\n",
    "    adddatecolumns(dataall,pfall)    \n",
    "    return dataall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "selected = ['CSERVICE','CRECEIVERPROTOCOL','CSENDERPROTOCOL','CSENDERENDPOINTID','CRECEIVERENDPOINTID','CGLOBALMESSAGEID','CSTARTTIME','CMESSAGETAT2','CSLATAT','CINBOUNDSIZE','CSTATUS']\n",
    "\n",
    "def getpfall(df,selected) :\n",
    "    #pfall = df.limit(5000000).toPandas()  \n",
    "    pfall = df.toPandas() \n",
    "    for each in selected:\n",
    "        pfall[each] = pfall[each].astype(str)\n",
    "    if len(pfall) == 0:\n",
    "        return pfall,0,0\n",
    "    return pfall, int(pfall.loc[pfall['CSTARTTIME'].astype(int).idxmax()]['CSTARTTIME']),len(pfall.index)        \n",
    "\n",
    "def gettest(to,selected) :\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME')    \n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "def getdata_lt(to,selected) :\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME') \n",
    "    #dataall = getpf(df2)    \n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "    \n",
    "def getdata_gt(_from,_diff,selected) :\n",
    "    to = _from + _diff\n",
    "    df2 = df.withColumn('CSTARTTIME', col('CSTARTTIME').cast('long')).filter(col(\"CSTARTTIME\") > _from ).filter(col(\"CSTARTTIME\") < to ).select(selected).dropDuplicates().orderBy('CSTARTTIME') \n",
    "    #dataall = getpf(df2)\n",
    "    #return dataall\n",
    "    return getpfall(df2,selected)\n",
    "\n",
    "start_lt = 1577041508174\n",
    "start_gt = [1577041507723,1580032925655,1581929401680,1583136852562]\n",
    "timestamp_diff = 1000000000\n",
    "\n",
    "dataall2 = getdata_lt(start_lt,selected)\n",
    "dataall = dataall2[0]\n",
    "\n",
    "while True:\n",
    "    dataall2 = getdata_gt(dataall2[1] ,timestamp_diff,selected)\n",
    "    if dataall2[2] == 0:\n",
    "        break  \n",
    "    dataall = dataall.append(dataall2[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfall = getpf(dataall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages sent by endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,2)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = createData_ym(pfall,0,1)\n",
    "createBarplot(md,24,9,3.0,title=\"number messages with errors sent by all endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details for different CSENDERENDPOINTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(mdcountsall[1].index.get_level_values(0))\n",
    "TOP=500000\n",
    "result = pfall.groupby(['CSENDERENDPOINTID']).count()\n",
    "data2 = pd.DataFrame()\n",
    "data2['date'] = result.index.get_level_values(0).astype(str)\n",
    "data2['outcome'] =  result['outcome'].astype(int)\n",
    "topsender =  data2[data2['outcome'] > TOP].sort_values('outcome').reset_index()\n",
    "topsender.columns = ['index', 'CSENDERENDPOINTID', 'outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall1 = pfall[pfall['CSENDERENDPOINTID'].isin(topsender['date'])]\n",
    "pfall1 = pfall[pfall['CSENDERENDPOINTID']==int(topsender.iloc[7]['CSENDERENDPOINTID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = str(len(topsender)) + \" senders with more than \" + str(TOP) + \" messages (kernel density estimate )\"\n",
    "ax = createKDE(data2,16,8,2,title)\n",
    "label(ax,1,90)\n",
    "#topsender.iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages of selected endpoint (Msgs / h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": number messages so far = \" + str(topsender.iloc[7]['outcome']))\n",
    "        \n",
    "\n",
    "def createHeatmapSeries(pfall, months,category, outcome) :\n",
    "    for month in months:\n",
    "        data2 = createData(pfall,month,outcome)\n",
    "        piv = pd.pivot_table(data2, values=\"outcome\",index=[\"hours\"], columns=[\"days\"], fill_value=0)\n",
    "        #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "        titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "        sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "        createHeatmap(piv, titlestring)\n",
    "       \n",
    "createHeatmapSeries(pfall1,[10,11,12,1,2,3,4],'messages', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number messages with errors of selected endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createHeatmapSeries(pfall1, [11,12,1,2,3,4],'messages with errors', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with LSTM Autoencoders (selected SENDERPOINTID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createDataframe(pfall1)\n",
    "train, test = getTrainAndTest(df,0.95)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[[OUTCOME]])\n",
    "train[OUTCOME] = scaler.transform(train[[OUTCOME]])\n",
    "test[OUTCOME] = scaler.transform(test[[OUTCOME]])\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[[OUTCOME]], train.close, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[[OUTCOME]], test.close, TIME_STEPS)\n",
    "print(X_train.shape)\n",
    "\n",
    "model = initmodel()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False)\n",
    "\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_mae_loss, bins=50, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df = testScoreDF(model, 1.5)\n",
    "anomalies     = test_score_df[test_score_df.anomaly == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anomalies.head()\n",
    "#anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "  label='msg count'\n",
    ");\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "  anomalies.index,\n",
    "  scaler.inverse_transform(anomalies.close),\n",
    "  color=sns.color_palette()[3],\n",
    "  s=152,\n",
    "  label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.legend();\n",
    "\n",
    "label(ax,5,80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
