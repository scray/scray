{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo: loads file with all messages (CSTARTTIME, CSENDERENDPOINTID, ymdhm )\n",
    "# show some charts, anomaly detection with LSTM autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base.dfBasics as dfBasics\n",
    "import base.common as common\n",
    "import base.encoder as encoder\n",
    "import base.pfAdapt as pfAdapt\n",
    "#import base.charts as charts\n",
    "#import base.anomaly as anomaly\n",
    "\n",
    "import pandas as pd    \n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = dfBasics.getSparkSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_parquet('/home/jovyan/work/output/sla_enc_all_4.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#senders = pd.unique(df['CSENDERENDPOINTID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "## Setup charts\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "def get_ym_string(a,b) :\n",
    "    return a + \"-\" + b\n",
    "    #return a.join([\"-\",b]) \n",
    "\n",
    "def get_ym(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    return a,b\n",
    "\n",
    "def get_ymd(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    return a,b,c\n",
    "\n",
    "def make_2digits(blist):\n",
    "    for n, b in enumerate(blist):\n",
    "        if int(b) < 10:\n",
    "             blist[n] = '0' + str(b)\n",
    "    return blist\n",
    "\n",
    "def get_ymd_string(a,b,c) :\n",
    "    if isinstance(a, str) :\n",
    "        return a + \"-\" + make_2digits([b])[0] + \"-\" + make_2digits([c])[0] \n",
    "    elif isinstance(a,pd.core.series.Series):\n",
    "        return a.astype(str) + \"-\" + make_2digits(b.astype(str)) + \"-\" + make_2digits(c.astype(str))\n",
    "    return a + \"-\" + pd.Index(make_2digits(b.tolist())) + \"-\" + pd.Index(make_2digits(c.tolist())) \n",
    "\n",
    "def get_ymdh(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    return a,b,c,d\n",
    "\n",
    "def get_ymdhm(mdcountsall) :\n",
    "    a = mdcountsall.index.get_level_values(0).astype(str)\n",
    "    b = mdcountsall.index.get_level_values(1).astype(str)\n",
    "    c = mdcountsall.index.get_level_values(2).astype(str)\n",
    "    d = mdcountsall.index.get_level_values(3).astype(str)\n",
    "    e = mdcountsall.index.get_level_values(4).astype(str)\n",
    "    return a,b,c,d,e\n",
    "\n",
    "def get_ymdh_string(a,b,c,d) :\n",
    "    if isinstance(a, str) :\n",
    "        return a + \"-\" + make_2digits([b])[0] + \"-\" + make_2digits([c])[0] + \"-\" + make_2digits([d])[0]\n",
    "    elif isinstance(a,pd.core.series.Series):\n",
    "        return a.astype(str) + \"-\" + make_2digits(b.astype(str)) + \"-\" + make_2digits(c.astype(str)) + \"-\" + make_2digits(d.astype(str))\n",
    "    return a + \"-\" + pd.Index(make_2digits(b.tolist())) + \"-\" + pd.Index(make_2digits(c.tolist())) + \"-\" + pd.Index(make_2digits(d.tolist()))\n",
    "\n",
    "def get_ymdhm_string(a,b,c,d,e) :\n",
    "    if isinstance(a, str) :\n",
    "        return a + \"-\" + make_2digits([b])[0] + \"-\" + make_2digits([c])[0] + \"-\" + make_2digits([d])[0] + \"-\" + make_2digits([e])[0]\n",
    "    elif isinstance(a,pd.core.series.Series):\n",
    "        return a.astype(str) + \"-\" + make_2digits(b.astype(str)) + \"-\" + make_2digits(c.astype(str)) + \"-\" + make_2digits(d.astype(str)) + \"-\" + make_2digits(e.astype(str))\n",
    "    return a + \"-\" + pd.Index(make_2digits(b.tolist())) + \"-\" + pd.Index(make_2digits(c.tolist())) + \"-\" + pd.Index(make_2digits(d.tolist())) + \"-\" + pd.Index(make_2digits(e.tolist()))\n",
    "\n",
    "\n",
    "def createData_ym(pfall,month) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['month'] == month)].groupby(['year','month'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month'])['year'].count()    \n",
    "    a,b = get_ym(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ym_string(a,b)\n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData(pfall,month=-1,year=2020,outcome='outcome') :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)].groupby(['year','month','day','hour'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day','hour'])['year'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2[outcome] =  mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int)\n",
    "\n",
    "    #for pivot table\n",
    "    data2['hours'] =  d.astype(int) \n",
    "    data2['days']  =  c.astype(int) \n",
    "    return data2\n",
    "\n",
    "\n",
    "def createData_ymd(pfall,month,year=2020) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)].groupby(['year','month','day'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day'])['year'].count()    \n",
    "    a,b,c = get_ymd(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymd_string(a,b,c)\n",
    "    data2['year'] = a.astype(int) \n",
    "    data2['month'] = b.astype(int) \n",
    "    data2['day'] = c.astype(int) \n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ymdh(pfall,month,year=2020) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)].groupby(['year','month','day','hour'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day','hour'])['year'].count()    \n",
    "    a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['year'] = a.astype(int) \n",
    "    data2['month'] = b.astype(int) \n",
    "    data2['day'] = c.astype(int) \n",
    "    data2['hour'] = d.astype(int)\n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "def createData_ymdhm(pfall,month,year=2020) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)].groupby(['year','month','day','hour','minute'])['year'].count()\n",
    "    else :\n",
    "        mdcountsall = pfall.groupby(['year','month','day','hour','minute'])['year'].count()    \n",
    "    a,b,c,d,e = get_ymdhm(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(a,b,c,d)\n",
    "    data2['year'] = a.astype(int) \n",
    "    data2['month'] = b.astype(int) \n",
    "    data2['day'] = c.astype(int) \n",
    "    data2['hour'] = d.astype(int)\n",
    "    data2['minute'] = e.astype(int)\n",
    "    data2['outcome'] = mdcountsall.reset_index(level=0, drop=True).reset_index()['year'].astype(int) \n",
    "    return data2\n",
    "\n",
    "\n",
    "def createData_column_ymdh(pfall,month=-1,year=2020, column=None) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)]\n",
    "    else :\n",
    "        mdcountsall = pfall \n",
    "    #a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymdh_string(pfall['year'], pfall['month'], pfall['day'],pfall['hour'])\n",
    "    df2 = mdcountsall[['year', 'month', 'day','hour',column]].copy()\n",
    "    data2 = pd.concat([data2, df2], axis=1)\n",
    "    data2.columns = list(data2.columns[:-1]) + ['outcome']\n",
    "    \n",
    "    return data2\n",
    "\n",
    "\n",
    "def createData_column_ymd(pfall,month=-1,year=2020, column=None) :\n",
    "    if (month > 0) & (month < 13) :\n",
    "        mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)]\n",
    "    else :\n",
    "        mdcountsall = pfall \n",
    "    #a,b,c,d = get_ymdh(mdcountsall)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['date'] = get_ymd_string(pfall['year'], pfall['month'], pfall['day'])\n",
    "    df2 = mdcountsall[['year', 'month', 'day',column]].copy()\n",
    "    data2 = pd.concat([data2, df2], axis=1)\n",
    "    data2.columns = list(data2.columns[:-1]) + ['outcome']\n",
    "    \n",
    "    return data2\n",
    "\n",
    "\n",
    "def label(graph,skip,rot) :\n",
    "    #print(len(graph.get_xticklabels()))\n",
    "    for ind, label in enumerate(graph.get_xticklabels()):\n",
    "        if ind % skip == 0:  # every 10th label is kept\n",
    "            label.set_visible(True)\n",
    "            label.set_rotation(rot)\n",
    "        else:\n",
    "            label.set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_skip(a):\n",
    "    print(a)\n",
    "    b=a\n",
    "    if a > 12:\n",
    "        b = (a - 12) / 12\n",
    "    return int(b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBarplot(md=None,fx=24,fy=12,fontscale=3.0,title=\"\") :\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=fontscale)\n",
    "    plt.figure(figsize=(fx,fy))\n",
    "    plt.title(title)\n",
    "    ax = sns.barplot(x=md['date'], y=md['outcome'], data=md)\n",
    "    label(ax,label_skip(len(ax.get_xticklabels())),75)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=75 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "## heatmap\n",
    "def createHeatmap(piv,title=\"\") :\n",
    "    plt.figure(figsize=(24,8))\n",
    "    plt.title(title)\n",
    "    ax = sns.heatmap(piv, square=True)\n",
    "    plt.setp( ax.xaxis.get_majorticklabels(), rotation=0 )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall\n",
    "#pfall5 = createData_column_ymdh(pfall,column='CMESSAGETAT2')\n",
    "#get_ymdh_string(pfall5['year'], pfall5['month'], pfall5['day'],pfall5['hour'])\n",
    "#pfall5['year']\n",
    "#pfall5['year'].astype(str) + '_' + make_2digits(pfall5['month'].astype(str))\n",
    "#make_2digits([c])[0] \n",
    "\n",
    "#pfall5.columns = pfall5.columns[:-1] + 'outcome'\n",
    "#pfall5.columns = list(pfall5.columns[:-1]) + ['outcome']\n",
    "#pfall5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year=2022\n",
    "#month=7\n",
    "#mdcountsall = pfall[(pfall['year'] == year) & (pfall['month'] == month)].groupby(['year','month','day','hour'])['year'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checka(pfall[(pfall[year]== 2022) & (pfall[month]== 7)])\n",
    "#pfall[(pfall[year]== '2022') & (pfall[month]== '7')]\n",
    "#pfall[(pfall[year] == 2022)] \n",
    "#pfall\n",
    "#pfall5 = createData_column_ymdh(pfall,column='CMESSAGETAT2')\n",
    "#pfall6 = checka(pfall5)\n",
    "#pd.unique(pfall5['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import monthrange\n",
    "\n",
    "def get_month(pda, year=2020, month=1):\n",
    "    return pda[(pda['month'] == month) & (pda['year'] == year)]\n",
    "\n",
    "def is_complete(pda, year=2020, month=1):\n",
    "    mm = pda[(pda['month'] == month) & (pda['year'] == year)]\n",
    "    if len(mm) > 0:\n",
    "        return monthrange(year, month)[1] == len(mm)\n",
    "    return None\n",
    "    \n",
    "def check_complete(pda, year=2020):\n",
    "    for m in range(1, 13):\n",
    "        mm = pda[(pda['month'] == m) & (pda['year'] == year)]\n",
    "        if len(mm) > 0:\n",
    "            print(m,monthrange(year, m)[1] == len(mm))\n",
    "\n",
    "            \n",
    "def add_line_to_dataframe(df=None,year=None,month=None,day=None,hour=None,minute=None,value=0): \n",
    "    \n",
    "    if hour is not None:\n",
    "        if minute is not None:\n",
    "            _date = get_ymdhm_string(str(year),str(month),str(day),str(hour),str(minute)) \n",
    "            df.loc[len(df)]=[_date,year,month,day,hour,minute,value]  \n",
    "        else:    \n",
    "            _date = get_ymdh_string(str(year),str(month),str(day),str(hour)) \n",
    "            df.loc[len(df)]=[_date,year,month,day,hour,value]   \n",
    "    else:    \n",
    "        _date = get_ymd_string(str(year),str(month),str(day))\n",
    "        df.loc[len(df)]=[_date,year,month,day,value]     \n",
    "    \n",
    "def check_and_fill_hours_of_day(df=None,year=2022,month=7,day=None):\n",
    "    _df=df[(df['year']==year) & (df['month']==month) & (df['day']==day)] \n",
    "    #print(_df)\n",
    "    \n",
    "    minutes = False\n",
    "    if 'minute' in df.columns:\n",
    "        minutes = True\n",
    "    \n",
    "    for hour in range(0, 24):\n",
    "        if len(_df[_df['hour']==hour]) == 0:\n",
    "            if minutes == True:\n",
    "                for minute in range(0, 59):\n",
    "                    #_date = get_ymdhm_string(str(year),str(month),str(day),str(hour),str(minute))  \n",
    "                    add_line_to_dataframe(df=df,year=year,month=month,day=day,hour=hour,minute=minute) \n",
    "            else:\n",
    "                #_date = get_ymdh_string(str(year),str(month),str(day),str(hour))  \n",
    "                add_line_to_dataframe(df=df,year=year,month=month,day=day,hour=hour) \n",
    "        else:\n",
    "            if minutes == True:\n",
    "                _df2 = _df[_df['hour']==hour]\n",
    "                for minute in range(0, 59):\n",
    "                    if len(_df2[_df2['minute']==minute]) == 0:\n",
    "                        add_line_to_dataframe(df=df,year=year,month=month,day=day,hour=hour,minute=minute)\n",
    "            else:\n",
    "                pass\n",
    "    #print (df)        \n",
    "    return df     \n",
    "    \n",
    "    \n",
    "#pda :  date,year,month,day.outcome \n",
    "def fill(pda,year,month,min_day=1,max_day=-1,hours=False): \n",
    "    if max_day == -1:\n",
    "        days = monthrange(year, month)[1]\n",
    "    else:\n",
    "        days = max_day\n",
    "        \n",
    "    #print(min_day,days,max_day,hours)     \n",
    "    minutes = False\n",
    "    if 'minute' in pda.columns:\n",
    "        minutes = True\n",
    "        \n",
    "    df = pda[(pda['month'] == month) & (pda['year'] == year)]\n",
    "    for day in range(min_day, days + 1):\n",
    "        if len(df[df['day']==day]) == 0:\n",
    "            #print(day)\n",
    "            if hours == True:\n",
    "                for hour in range (0,24):\n",
    "                    if minutes == True:\n",
    "                        for minute in range(0, 59): \n",
    "                            add_line_to_dataframe(df=pda,year=year,month=month,day=day,hour=hour,minute=minute) \n",
    "                    else:\n",
    "                        add_line_to_dataframe(df=pda,year=year,month=month,day=day,hour=hour) \n",
    "            else:    \n",
    "                add_line_to_dataframe(df=pda,year=year,month=month,day=day)\n",
    "        else:\n",
    "            if hours == True:\n",
    "                #print(year,month,day)\n",
    "                check_and_fill_hours_of_day(df=pda,year=year,month=month,day=day)\n",
    "       \n",
    "    \n",
    "def check(pda):\n",
    "    _start_year = min(pd.unique(pda['year']))\n",
    "    _end_year = max(pd.unique(pda['year']))  \n",
    "    _start_month = min(pd.unique(pda[pda['year']== _start_year]['month']))\n",
    "    _end_month = max(pd.unique(pda[pda['year']== _end_year]['month']))\n",
    "    _end = 13\n",
    "    \n",
    "    if 'hour' in pda.columns:\n",
    "        hours = True\n",
    "    else:\n",
    "        hours = False\n",
    "        \n",
    "    for year in range(_start_year,_end_year+1):   \n",
    "        for month in range(_start_month,_end):        \n",
    "            #print(year,month, _end_month, _end_year)\n",
    "            \n",
    "            if((month == _start_month) & (year == _start_year)):\n",
    "                min_day = (min(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "            else:\n",
    "                min_day = 1\n",
    "                \n",
    "            if((month == _end_month) & (year == _end_year)):\n",
    "                max_day = (max(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "                #print(month,year,max_day)\n",
    "                fill(pda,year,month,min_day=min_day, max_day=max_day,hours=hours)\n",
    "            else:    \n",
    "                fill(pda,year,month,min_day=min_day,hours=hours)\n",
    "            if (year == _end_year) &  (month == _end_month):\n",
    "                #print('DONE')\n",
    "                break    \n",
    "        _start_month=1\n",
    "            \n",
    "        if year == (_end_year-1):\n",
    "            _end = _end_month + 1     \n",
    "            \n",
    "        if (year == _end_year) &  (month == _end_month):\n",
    "            #print('DONE')\n",
    "            break       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019-10-29-00\n",
    "# 2022-10-06-23\n",
    "\n",
    "def mean_column(column):\n",
    "    return int(round(column.mean(),0))\n",
    "\n",
    "def check_and_fill_hours_of_daya(df=None,year=2022,month=7,day=None, outcome='outcome'):\n",
    "    _df=df[(df['year']==year) & (df['month']==month) & (df['day']==day)] \n",
    "    \n",
    "    df=pd.DataFrame(columns=['date', 'year', 'month', 'day', 'hour', outcome])\n",
    "    for hour in range(0, 24):\n",
    "        _hours = _df[_df['hour']==hour]\n",
    "        if len(_df[_df['hour']==hour]) == 0:\n",
    "            _date = get_ymdh_string(str(year),str(month),str(day),str(hour))  \n",
    "            add_line_to_dataframe(df=df,year=year,month=month,day=day,hour=hour) \n",
    "            pass\n",
    "        else:\n",
    "            add_line_to_dataframe(df=df,year=year,month=month,day=day,hour=hour,value=mean_column(_hours[outcome]))\n",
    "    #print (df)      \n",
    "    return df \n",
    "\n",
    "#pda :  date,year,month,day.outcome \n",
    "def filla(pda,year,month,min_day=1,max_day=-1,hours=False, outcome='outcome'): \n",
    "    if max_day == -1:\n",
    "        days = monthrange(year, month)[1]\n",
    "    else:\n",
    "        days = max_day\n",
    "        \n",
    "    #df = pda[(pda['month'] == month) & (pda['year'] == year)]\n",
    "    if hours == True:\n",
    "        df=pd.DataFrame(columns=['date', 'year', 'month', 'day', 'hour', outcome])\n",
    "    else:\n",
    "        df=pd.DataFrame(columns=['date', 'year', 'month', 'day', outcome])\n",
    "        \n",
    "    for day in range(min_day, days + 1):\n",
    "        if hours == True:\n",
    "            df_b = check_and_fill_hours_of_daya(df=pda,year=year,month=month,day=day,outcome=outcome)\n",
    "            df = pd.concat([df, df_b], ignore_index=True)   \n",
    "        else:\n",
    "            _df=pda[(pda['year']==year) & (pda['month']==month) & (pda['day']==day)]\n",
    "            if len(_df) > 0 :\n",
    "                #print(len(_df),mean_column(_df['outcome']), list(_df['outcome']))\n",
    "                value=mean_column(_df[outcome])\n",
    "            else:\n",
    "                value=0\n",
    "            add_line_to_dataframe(df=df,year=year,month=month,day=day,value=value) \n",
    "    return df   \n",
    "            \n",
    "    \n",
    "def checka(pda, outcome='outcome'):\n",
    "    _start_year = min(pd.unique(pda['year']))\n",
    "    _end_year = max(pd.unique(pda['year']))  \n",
    "    _start_month = min(pd.unique(pda[pda['year']== _start_year]['month']))\n",
    "    _end_month = max(pd.unique(pda[pda['year']== _end_year]['month']))\n",
    "    _end = 13\n",
    "    \n",
    "    if 'hour' in pda.columns:\n",
    "        hours = True\n",
    "        df=pd.DataFrame(columns=['date', 'year', 'month', 'day', 'hour', outcome])\n",
    "    else:\n",
    "        hours = False\n",
    "        df=pd.DataFrame(columns=['date', 'year', 'month', 'day', outcome])\n",
    "    \n",
    "    \n",
    "    for year in range(_start_year,_end_year+1):   \n",
    "        for month in range(_start_month,_end):        \n",
    "            #print(year,month, _end_month, _end_year)\n",
    "            \n",
    "            if((month == _start_month) & (year == _start_year)):\n",
    "                min_day = (min(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "            else:\n",
    "                min_day = 1\n",
    "                \n",
    "            if((month == _end_month) & (year == _end_year)):\n",
    "                max_day = (max(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "                #print(month,year,max_day)\n",
    "                df_b = filla(pda,year,month,min_day=min_day, max_day=max_day,hours=hours,outcome=outcome)\n",
    "                df = pd.concat([df, df_b], ignore_index=True)\n",
    "            else:    \n",
    "                df_b =  filla(pda,year,month,min_day=min_day,hours=hours,outcome=outcome)\n",
    "                df = pd.concat([df, df_b], ignore_index=True)\n",
    "            if (year == _end_year) &  (month == _end_month):\n",
    "                #print('DONE')\n",
    "                break    \n",
    "        _start_month=1\n",
    "            \n",
    "        if year == (_end_year-1):\n",
    "            _end = _end_month + 1     \n",
    "            \n",
    "        if (year == _end_year) &  (month == _end_month):\n",
    "            #print('DONE')\n",
    "            break       \n",
    "    return df        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_list(pda, hours=False):\n",
    "    _start_year = min(pd.unique(pda['year']))\n",
    "    _end_year = max(pd.unique(pda['year']))  \n",
    "    _start_month = min(pd.unique(pda[pda['year']== _start_year]['month']))\n",
    "    _end_month = max(pd.unique(pda[pda['year']== _end_year]['month']))\n",
    "    _end = 13\n",
    "    \n",
    "    _date_list = []\n",
    "    \n",
    "    for year in range(_start_year,_end_year+1):   \n",
    "        for month in range(_start_month,_end):        \n",
    "            #print(year,month, _end_month, _end_year)\n",
    "            \n",
    "            if((month == _start_month) & (year == _start_year)):\n",
    "                min_day = (min(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "            else:\n",
    "                min_day = 1\n",
    "            \n",
    "            if((month == _end_month) & (year == _end_year)):\n",
    "                days = (max(list(pda[(pda['month'] == month) & (pda['year'] == year)]['day'])))\n",
    "            else:\n",
    "                days = monthrange(year, month)[1]\n",
    "            \n",
    "            for day in range(min_day, days + 1):\n",
    "                if hours == True:\n",
    "                    for hour in range(0, 24):\n",
    "                        _date = get_ymdh_string(str(year),str(month),str(day),str(hour)) \n",
    "                        _date_list.append(_date) \n",
    "                else:\n",
    "                    _date = get_ymd_string(str(year),str(month),str(day))\n",
    "                    _date_list.append(_date)    \n",
    "            \n",
    "            if (year == _end_year) &  (month == _end_month):\n",
    "                #print('DONE')\n",
    "                break    \n",
    "        _start_month=1\n",
    "            \n",
    "        if year == (_end_year-1):\n",
    "            _end = _end_month + 1     \n",
    "            \n",
    "        if (year == _end_year) &  (month == _end_month):\n",
    "            #print('DONE')\n",
    "            break       \n",
    "    return _date_list        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_fill_hours_of_month(dataframe=None,year=2022,month=7):\n",
    "    \n",
    "    days=pd.unique(dataframe['days'])\n",
    "    _start_day = min(days)\n",
    "    _end_day   = max(days)  \n",
    "\n",
    "    for day in range(_start_day,_end_day+1):\n",
    "        _df=dataframe[dataframe['days']==day] \n",
    "        for hour in range(0, 24):\n",
    "            if len(_df[_df['hours']==hour]) == 0:\n",
    "                _date = get_ymdh_string(str(year),str(month),str(day),str(hour))  \n",
    "                #print([_date,0,hour,day])\n",
    "                dataframe.loc[len(dataframe)]=[_date,0,hour,day]\n",
    "    return dataframe       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complete(dataframe=None,year=2020):\n",
    "    for m in range(1, 13):\n",
    "        #print(m)\n",
    "        mm = dataframe[(dataframe['month'] == m) & (dataframe['year'] == year)]\n",
    "        print(m,monthrange(year, m)[1] == len(mm))\n",
    "\n",
    "def fill1(pda,year,month):        \n",
    "    days = monthrange(year, month)[1]\n",
    "    df = pda[(pda['month'] == month) & (pda['year'] == year)]\n",
    "    for day in range(1, days + 1):\n",
    "        if len(df[df['day']==day]) == 0:\n",
    "            pda.loc[len(pda)]=[get_ymd_string(str(year),str(month),str(day)),year,month,day,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(pfall):\n",
    "    return pd.unique(pfall['CRECEIVERENDPOINTID']), pd.unique(pfall['CSTATUS']), pd.unique(pfall['CSERVICE']), pd.unique(pfall['CSENDERPROTOCOL']), pd.unique(pfall['CRECEIVERPROTOCOL'])\n",
    "\n",
    "#un=unique(pfall)\n",
    "#unique(pfall[pfall['CRECEIVERENDPOINTID']==725])\n",
    "\n",
    "#pfall0 = pfall[pfall['CRECEIVERENDPOINTID']==un[0][1]]\n",
    "#pda = createData_ymd(pfall0,0)\n",
    "\n",
    "#del pda['index'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time(object):         \n",
    "    def __init__(self,year=None,month=None,day=None):\n",
    "        self.year  = self._int_value(year)\n",
    "        self.month = self._int_value(month)\n",
    "        self.day   = self._int_value(day)\n",
    "    \n",
    "    def _int_value(self,value):\n",
    "        if value == '' or value == None:\n",
    "            return None\n",
    "        return int(value)\n",
    "        \n",
    "\n",
    "class TimeRange(object):    \n",
    "    def __init__(self,dataframe=None,year_from=None,month_from=None,day_from=None,year_to=None,month_to=None,day_to=None):\n",
    "        if dataframe is None:\n",
    "            self.start = Time(year=year_from,month=month_from,day=day_from).__dict__\n",
    "            self.end   = Time(year_to,month_to,day_to).__dict__\n",
    "        else:\n",
    "            #print(dataframe)\n",
    "            year_from = min(pd.unique(dataframe['year']))\n",
    "            year_to = max(pd.unique(dataframe['year']))  \n",
    "            month_from = min(pd.unique(dataframe[dataframe['year']== year_from]['month']))\n",
    "            month_to = max(pd.unique(dataframe[dataframe['year']== year_to]['month']))\n",
    "            day_from = min(pd.unique(dataframe[(dataframe['year']== year_from) & (dataframe['month'] == month_from)]['day']))\n",
    "            day_to = max(pd.unique(dataframe[(dataframe['year']== year_to) & (dataframe['month'] == month_to)]['day']))\n",
    "            self.start = Time(year_from,month_from,day_from).__dict__\n",
    "            self.end   = Time(year_to,month_to,day_to).__dict__\n",
    "            \n",
    "#TimeRange(dataframe=pfall).__dict__  \n",
    "\n",
    "def set_date_widget_value(element, value):\n",
    "    if value != None:\n",
    "        element.value = str(value)\n",
    "    else:\n",
    "        element.value = ''\n",
    "\n",
    "def init_date_widget(_res):\n",
    "    _range = TimeRange(dataframe=_res)\n",
    "    set_date_widget_value(year_from, _range.start['year'])\n",
    "    set_date_widget_value(month_from, _range.start['month'])\n",
    "    set_date_widget_value(day_from, _range.start['day'])\n",
    "    \n",
    "    set_date_widget_value(year_to, _range.end['year'])\n",
    "    set_date_widget_value(month_to, _range.end['month'])\n",
    "    set_date_widget_value(day_to, _range.end['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHeatmapPfall(pfall=None,sender='all',month=1,year=2020,values='outcome',index='hours',columns='days'):\n",
    "    #print(month,year)\n",
    "    global data2\n",
    "    global piv\n",
    "    data2 = createData(pfall,month,year=year)\n",
    "    piv = pd.pivot_table(data2, values=values,index=[index], columns=[columns], fill_value=0)\n",
    "    #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category + \" so far = \" + str(topsender.iloc[7]['outcome']) + \" , month: \" + str(month) \n",
    "    #titlestring = \"CSENDERENDPOINTID: \" + str(topsender.iloc[7]['CSENDERENDPOINTID']) + \": \"+ category  + \" month: \" + str(month) \n",
    "    titlestring =\"number messages \" + str(year) + \"-\" + str(month) + \"  \" + sender\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "    createHeatmap(piv, titlestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_value_submit_month(change):\n",
    "    #adapt_all()    \n",
    "    sender=0\n",
    "   \n",
    "    with out:\n",
    "        clear_output()\n",
    "        month=int(month_from.value)\n",
    "        year=int(year_from.value)\n",
    "        createHeatmapPfall(pfall0,\"endpoint \" + str(CSENDERENDPOINTID) + ' --> ' + str(CRECEIVERENDPOINTID),month=month, year=year)\n",
    "        #print(int(month_from.value), year_from.value)\n",
    "        #createHeatmapPfall(pfall=pfall,sender='all',month=1,year=2020)\n",
    "        \n",
    "        md2 = createData_ymd(pfall0,month,year=year)\n",
    "        ax=createBarplot(md2,fx=24,fy=12,fontscale=3.0,title=\"number messages \" + \"endpoint \" + str(CSENDERENDPOINTID) + ' --> ' + str(CRECEIVERENDPOINTID))\n",
    "        label(ax,1000,90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_index_by_date_column(df,column='date'):\n",
    "    df.set_index(df[column], inplace=True)\n",
    "    #del df['index']\n",
    "    #del df['date']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(pfall):\n",
    "    global pda_hour\n",
    "    global pda_CMESSAGETAT2_hour\n",
    "    \n",
    "    pfall1 = pfall.sort_values(['year','month','day','hour']).reset_index()\n",
    "    del pfall1['index']\n",
    "    del pfall1['CGLOBALMESSAGEID']\n",
    "\n",
    "    pfall0 = pfall\n",
    "\n",
    "    ####\n",
    "    pda_hour = createData_ymdh(pfall0,0)\n",
    "    check(pda_hour)\n",
    "    pda_hour = pda_hour.sort_values(['date']).reset_index()\n",
    "    _index = replace_index_by_date_column(pda_hour)\n",
    "\n",
    "    ####\n",
    "    #_column = 'CINBOUNDSIZE'\n",
    "    _column='CMESSAGETAT2'\n",
    "    pfall5 = createData_column_ymdh(pfall,column=_column)\n",
    "    pda_CMESSAGETAT2_hour = checka(pfall5 ).sort_values(['date'])\n",
    "    #pda_CINBOUNDSIZE_hour = checka(pfall5 ).sort_values(['date'])\n",
    "    _index = replace_index_by_date_column(pda_CMESSAGETAT2_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection with LSTM Autoencoders (selected sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_period(anomalyEnc):\n",
    "    return anomalyEnc.train.index[0], anomalyEnc.train.index[len(anomalyEnc.train.index)-1]\n",
    "\n",
    "def get_test_period(anomalyEnc):\n",
    "    return anomalyEnc.test.index[0], anomalyEnc.test.index[len(anomalyEnc.test) -1]\n",
    "\n",
    "def get_period(pfall,percent = 1.0):\n",
    "    max_index = int(len(pfall1) * percent)\n",
    "    return pfall.iloc[pfall.index[0]]['date'], pfall.iloc[pfall.index[max_index -1]]['date']\n",
    "\n",
    "def get_percent(pfall1,year=None,month=None,day=None,hour=None):\n",
    "    _pfall = pfall1[(pfall1['year'] == year) & (pfall1['month'] == month)]\n",
    "    if day != None:\n",
    "        _pfall = _pfall[(_pfall['day'] == day)]\n",
    "    if hour != None:\n",
    "        _pfall = _pfall[(_pfall['hour'] == hour)]\n",
    "    \n",
    "    index = _pfall.index[len(_pfall.index)-1]    \n",
    "    #print(index)\n",
    "    index = pfall1.index.get_loc(index)\n",
    "    \n",
    "    return (index + 1) / len(pfall1)\n",
    "    #return _pfall.index\n",
    "    \n",
    "def get_index_period(pfall):\n",
    "    max_index = int(len(pfall) * percent) -1\n",
    "    return pfall.index[0], pfall.index[max_index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import AnomalyDetectionLSTMAutoencoder\n",
    "\n",
    "def train_model(dataframe=None, time_steps=30, year=2022,month=6,day=None,hour=None):\n",
    "    anomalyEnc = AnomalyDetectionLSTMAutoencoder.AnomalyDetectionLSTMAutoencoder(TIME_STEPS = time_steps)\n",
    "    #anomalyEnc.initAndTrain_divide(dataframe, get_percent(dataframe,year=year,month=month,day=day,hour=hour))\n",
    "    \n",
    "    dataframe['datetime']  = pd.to_datetime(dataframe[[\"year\", \"month\", \"day\", \"hour\"]])\n",
    "    \n",
    "    anomalyEnc.df1 = anomalyEnc.createDataframe(dataframe)\n",
    "    anomalyEnc.df1['datetime'] = dataframe['datetime']\n",
    "    \n",
    "    perc_train = get_percent(dataframe,year=year,month=month,day=day,hour=hour)\n",
    "    train, test = anomalyEnc.getTrainAndTest(anomalyEnc.df1,perc_train)\n",
    "    anomalyEnc.initAndTrain(train=train, test=test)\n",
    "    \n",
    "    return anomalyEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies(anomalyEnc,threshold):\n",
    "    anomalyEnc.evaluateAnomalies(threshold)\n",
    "    anomalyEnc.anomalies['datetime'] = anomalyEnc.test['datetime']\n",
    "    return anomalyEnc.anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_dataframe(b):\n",
    "    _df = pd.DataFrame()\n",
    "    _df['outcome'] = b.reshape([1, len(b)])[0]\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models():\n",
    "    global anomalyEnc_hour\n",
    "    global anomalyEnc_CMESSAGETAT2_hour\n",
    "    anomalyEnc_hour = train_model(dataframe=pda_hour,time_steps=24, year=2020,month=12)\n",
    "    _anomalies = get_anomalies(anomalyEnc_hour,0.9).index\n",
    "\n",
    "    anomalyEnc_CMESSAGETAT2_hour = train_model(dataframe=pda_CMESSAGETAT2_hour,time_steps=24, year=2020,month=12)\n",
    "    _anomalies = get_anomalies(anomalyEnc_CMESSAGETAT2_hour,0.9).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE scaler, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/how-to-save-and-load-models-and-data-preparation-in-scikit-learn-for-later-use/\n",
    "#anomalyEnc2.test.head()\n",
    "#anomalyEnc2.scaler\n",
    "\n",
    "from pickle import dump\n",
    "# save the model\n",
    "#dump(model, open('model.pkl', 'wb'))\n",
    "# save the scaler\n",
    "\n",
    "def save_models():\n",
    "    from pathlib import Path\n",
    "    _path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_hour/'\n",
    "    _enc = anomalyEnc_hour\n",
    "    Path(_path).mkdir(parents=True, exist_ok=True)\n",
    "    dump(_enc.scaler, open(_path + 'scaler.pkl', 'wb'))\n",
    "    _enc.test.to_parquet(_path + 'test.parquet')\n",
    "\n",
    "    _path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_CMESSAGETAT2_hour/'\n",
    "    _enc = anomalyEnc_CMESSAGETAT2_hour\n",
    "    Path(_path).mkdir(parents=True, exist_ok=True)\n",
    "    dump(_enc.scaler, open(_path + 'scaler.pkl', 'wb'))\n",
    "    _enc.test.to_parquet(_path + 'test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(index, values,label):\n",
    "    return sns.scatterplot(\n",
    "      x=index,\n",
    "      y=values,\n",
    "      color=sns.color_palette()[3],\n",
    "      #s=152,\n",
    "      s=50,  \n",
    "      label=label\n",
    "      #linewidth=5  \n",
    "    )\n",
    "    \n",
    "def plot_test(test,scaler,anomalies,titlestring,xlabel):\n",
    "\n",
    "    fig = plt.figure(figsize=(18,9))\n",
    "    \n",
    "    plt.plot(\n",
    "      test[anomalyEnc.TIME_STEPS:].index, \n",
    "      #scaler.inverse_transform(test[TIME_STEPS:].close), \n",
    "      scaler.inverse_transform(test[anomalyEnc.TIME_STEPS:]),   \n",
    "      label='msg count'\n",
    "    );\n",
    "\n",
    "    #ax = scatterplot(anomalyEnc.anomalies.index, anomalyEnc.scaler.inverse_transform(anomalyEnc.anomalies['close']), 'anomaly')  \n",
    "    #ax = scatterplot(anomalyEnc.anomalies.index, anomalyEnc.scaler.inverse_transform(anomalyEnc.anomalies), 'anomaly')\n",
    "    _a=anomalyEnc.anomalies['close']\n",
    "    _a = pd.DataFrame(_a)\n",
    "    _a = anomalyEnc.scaler.inverse_transform(_a)\n",
    "    _a = _a[:, 0]\n",
    "    #print(_a,type(_a),_a[0],len(_a),len(_a[:, 0]))\n",
    "    ax = scatterplot(anomalyEnc.anomalies.index, _a, 'anomaly') \n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False) \n",
    "    plt.xticks(rotation=25)\n",
    "    plt.legend();\n",
    "\n",
    "    label(ax,5,80)\n",
    "    plt.title(titlestring)\n",
    "    plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problems / Erwartungsmonitoring\n",
    "\n",
    "- am 12.07.2022 wurde ein BIS Release installiert dass zur Verzögerung vieler Nachrichten geführt hat.\n",
    "Die letzten Nachrichten wurden am Folgetag um ca. 15:00 Uhr CET verarbeitet.\n",
    "Es sollten also deutliche Anomalien zwischen 12.07.2022 (10:00 Uhr) bis 13.07.2022 (15:00 Uhr) zu sehen sein.\n",
    "\n",
    "\n",
    "- 13.09.2022 19:00 Uhr / 20:00 Uhr UTC: Massive Last, Anflutung des Systems, vor allem im zweiten Intervall.\n",
    "Durchlaufzeiten waren deutlich erhöht. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(_df,contains=None, y='outcome'):\n",
    "    _df2 = _df[_df.index.str.contains(contains)]\n",
    "\n",
    "    fig = plt.figure(figsize=(18,9))\n",
    "    \n",
    "    hours= False\n",
    "    \n",
    "    plt.plot(_df2.index, _df2[y], label=y)\n",
    "    if len(_df2.index[0].split('-')) == 4:\n",
    "        hours = True\n",
    "        ticks=_df2.index[_df2.index.str.contains('-00')]\n",
    "        ticks2 = list(ticks)\n",
    "        ticks2.append('2022-07-12-10')\n",
    "        ticks2.append('2022-07-13-15')\n",
    "        ticks2.sort()\n",
    "        ticks = ticks2\n",
    "    elif len(_df2.index[0].split('-')) == 3:\n",
    "        ticks=list(_df2.index[_df2.index.str.contains('-01')])  \n",
    "        for day in range(2,10):\n",
    "            ticks.remove('2022-01-0' + str(day))\n",
    "        for day in range(10,32):\n",
    "            ticks.remove('2022-01-' + str(day)) \n",
    "        b=(len(ticks)-1)    \n",
    "        ticks = ticks[:6] + ['2022-07-12'] + ticks[6:b]     \n",
    "    else:\n",
    "        ticks=_df2.index\n",
    "    #ticks=[_df2.index[0]]\n",
    "    \n",
    "    #12.07.2022 (10:00 Uhr) bis 13.07.2022 (15:00 Uhr)\n",
    "    if hours == False:\n",
    "        plt.axvline(x='2022-07-12', color=\"red\")\n",
    "        plt.axvline(x='2022-09-13', color=\"red\")\n",
    "    else:    \n",
    "         plt.axvline(x='2022-07-12-10', color=\"red\")\n",
    "         plt.axvline(x='2022-07-13-15', color=\"red\")\n",
    "         #plt.axvline(x='2022-09-13-19', color=\"red\")\n",
    "         #plt.axvline(x='2022-09-13-20', color=\"red\")\n",
    "       \n",
    "    #print(ticks)\n",
    "    \n",
    "    plt.legend();\n",
    "    plt.xticks(rotation=80,ticks=ticks)\n",
    "    plt.show(fig)\n",
    "    return ticks    \n",
    "\n",
    "\n",
    "\n",
    "def plot_graph2(test=None, expect=None, column_y='outcome',contains=None, date_from=None, date_to=None):\n",
    "        \n",
    "    _test =test\n",
    "    \n",
    "    if contains is not None:\n",
    "        _test = _test[_test.index.str.contains(contains)]\n",
    "        #anomalies = anomalies[anomalies.index.str.contains(contains)]\n",
    "    elif date_from is not None and date_to is not None:\n",
    "        _from_index = _test.index.get_loc(date_from)\n",
    "        _to_index   = _test.index.get_loc(date_to)\n",
    "        #_df2 = _df2[_df2.index[_from_index:_to_index + 1]]\n",
    "        _test = _test.iloc[_from_index:_to_index + 1]\n",
    "    else:\n",
    "        pass    \n",
    "            \n",
    "    fig = plt.figure(figsize=(18,9))\n",
    "    plt.plot(_test.index, _test[column_y])       \n",
    "    \n",
    "    ticks = (_test.index[0], _test.index[len(_test.index) - 1])\n",
    "    plt.xticks(rotation=80,ticks=ticks)\n",
    "    \n",
    "    if expect is not None:\n",
    "        #print(_test.index.get_loc(expect[0]),_test.index.get_loc(expect[1]))\n",
    "        e1=_test.index.get_loc(expect[0])\n",
    "        e2=_test.index.get_loc(expect[1])\n",
    "        x = np.arange(e1,e2)\n",
    "        #x = np.arange(expect[0],expect[1])\n",
    "        _max = np.max(_test[column_y])\n",
    "        y1 = [0]*len(x)\n",
    "        y2 = [_max]*len(x)\n",
    "        plt.fill_between(x, y1, y2, facecolor='g', alpha=.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_anomalies(scaler=None,anomalies=None, column_y='close'):\n",
    "    _a = anomalies[column_y]\n",
    "    _a = pd.DataFrame(_a)\n",
    "    _a = scaler.inverse_transform(_a)\n",
    "    _a = _a[:, 0]\n",
    "    return _a\n",
    "\n",
    "def filtered_plot(test=None,scaler=None,anomalies=None,TIME_STEPS=None,contains=None, date_from=None, date_to=None,titlestring='',xlabel='',skip=None,rotate=80):\n",
    "    #_df2 = test[TIME_STEPS:]\n",
    "    _df2 = test\n",
    "    \n",
    "    if contains is not None:\n",
    "        _df2 = _df2[_df2.index.str.contains(contains)]\n",
    "        anomalies = anomalies[anomalies.index.str.contains(contains)]\n",
    "    elif date_from is not None and date_to is not None:\n",
    "        _from_index = _df2.index.get_loc(date_from)\n",
    "        _to_index   = _df2.index.get_loc(date_to)\n",
    "        #_df2 = _df2[_df2.index[_from_index:_to_index + 1]]\n",
    "        _df2 = _df2.iloc[_from_index:_to_index + 1]\n",
    "        \n",
    "        anomalies = anomalies.iloc[_from_index:_to_index + 1]\n",
    "        \n",
    "        for date in anomalyEnc6.anomalies.index:\n",
    "            if date >= '2022-07-01-00':\n",
    "                _min = anomalyEnc6.anomalies.index.get_loc(date)\n",
    "                break     \n",
    "        for _max in range(_min,len(anomalyEnc6.anomalies.index)):\n",
    "            if anomalyEnc6.anomalies.index[_max] > '2022-07-07-23':\n",
    "                break\n",
    "        anomalies = anomalies.iloc[_min:_max]         \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    _a = inverse_transform_anomalies(scaler=scaler,anomalies=anomalies, column_y='close')\n",
    "    \n",
    "    fig = plt.figure(figsize=(30,12))\n",
    "\n",
    "    plt.plot(\n",
    "      _df2.index,  \n",
    "      scaler.inverse_transform(_df2),   \n",
    "      label='msg count'\n",
    "    );\n",
    "\n",
    "    \n",
    "    \n",
    "    ax = scatterplot(anomalies.index, _a, 'anomaly') \n",
    "\n",
    "    if skip is not None:\n",
    "        label(ax,skip,rotate)\n",
    "    \n",
    "    hours= False\n",
    "    if len(_df2.index[0].split('-')) == 4:\n",
    "        hours = True\n",
    "    if hours == False:\n",
    "        plt.axvline(x='2022-07-12', color=\"red\")\n",
    "        #plt.axvline(x='2022-07-13', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13', color=\"red\")\n",
    "    else:    \n",
    "        plt.axvline(x='2022-07-12-10', color=\"red\")\n",
    "        plt.axvline(x='2022-07-13-15', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13-19', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13-20', color=\"red\")\n",
    "        \n",
    "    plt.title(titlestring)\n",
    "    #plt.show(fig)     \n",
    "        \n",
    "def filtered_plot_enc(anomalyEnc, contains=None,date_from=None, date_to=None,titlestring='',xlabel='',skip=None,rotate=80):     \n",
    "    return filtered_plot(anomalyEnc.test,anomalyEnc.scaler,anomalyEnc.anomalies,anomalyEnc.TIME_STEPS,contains,date_from, date_to,titlestring,xlabel,skip,rotate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_plot2(test=None,scaler=None,anomalies=None,TIME_STEPS=None,contains=None, date_from=None, date_to=None,titlestring='',xlabel='',skip=None,rotate=80, annotations=True, ylabel='msg count'):\n",
    "    #_df2 = test[TIME_STEPS:]\n",
    "    _df2 = test\n",
    "    \n",
    "    if contains is not None:\n",
    "        _df2 = _df2[_df2.index.str.contains(contains)]\n",
    "        anomalies = anomalies[anomalies.index.str.contains(contains)]\n",
    "    elif date_from is not None and date_to is not None:\n",
    "        _from_index = _df2.index.get_loc(date_from)\n",
    "        _to_index   = _df2.index.get_loc(date_to)\n",
    "        #_df2 = _df2[_df2.index[_from_index:_to_index + 1]]\n",
    "        _df2 = _df2.iloc[_from_index:_to_index + 1]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    _a = anomalies['close']\n",
    "    _a = pd.DataFrame(_a)\n",
    "    _a = scaler.inverse_transform(_a)\n",
    "    _a = _a[:, 0]\n",
    "    \n",
    "    fig = plt.figure(figsize=(30,12))\n",
    "\n",
    "    skip_x = label_skip(len(_df2.index))\n",
    "    \n",
    "    plt.plot(\n",
    "      _df2.index,  \n",
    "      scaler.inverse_transform(_df2),   \n",
    "      label=ylabel\n",
    "    );\n",
    "\n",
    "    plt.xticks( _df2.index[0::skip_x],rotation=70,fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    skip_x = label_skip(len(anomalies.index))\n",
    "    \n",
    "    ax = scatterplot(anomalies.index, _a, 'anomaly') \n",
    "\n",
    "    #if skip is not None:\n",
    "    #    label(ax,skip,rotate)\n",
    "    \n",
    "    hours= False\n",
    "    if len(_df2.index[0].split('-')) == 4:\n",
    "        hours = True\n",
    "        \n",
    "    if annotations == True:    \n",
    "        if hours == False:\n",
    "            plt.axvline(x='2022-07-12', color=\"red\")\n",
    "            #plt.axvline(x='2022-07-13', color=\"red\")\n",
    "            #plt.axvline(x='2022-09-13', color=\"red\")\n",
    "        else:    \n",
    "            plt.axvline(x='2022-07-12-10', color=\"red\")\n",
    "            plt.axvline(x='2022-07-13-15', color=\"red\")\n",
    "            #plt.axvline(x='2022-09-13-19', color=\"red\")\n",
    "            #plt.axvline(x='2022-09-13-20', color=\"red\")\n",
    "        \n",
    "    plt.title(titlestring)\n",
    "    #plt.show(fig)     \n",
    "        \n",
    "\n",
    "def filtered_plot_enc2(anomalyEnc, contains=None,date_from=None, date_to=None,titlestring='',xlabel='',skip=None,rotate=80, annotations=True, ylabel='msg count'):     \n",
    "    return filtered_plot2(anomalyEnc.test,anomalyEnc.scaler,anomalyEnc.anomalies,anomalyEnc.TIME_STEPS,contains,date_from, date_to,titlestring,xlabel,skip,rotate, annotations=annotations, ylabel=ylabel)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_add_anomaly(_df2):\n",
    "    hours= False\n",
    "    if len(_df2.index[0].split('-')) == 4:\n",
    "        hours = True\n",
    "    if hours == False:\n",
    "        plt.axvline(x='2022-07-12', color=\"red\")\n",
    "        #plt.axvline(x='2022-07-13', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13', color=\"red\")\n",
    "    else:    \n",
    "        plt.axvline(x='2022-07-12-10', color=\"red\")\n",
    "        plt.axvline(x='2022-07-13-15', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13-19', color=\"red\")\n",
    "        #plt.axvline(x='2022-09-13-20', color=\"red\")\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from datetime import time\n",
    "import datetime as dt\n",
    "\n",
    "def add_datetime_column(dataframe):\n",
    "    _date = []\n",
    "    for i,row in dataframe.iterrows():\n",
    "        _date.append(dt.datetime.strptime(i, \"%Y-%m-%d-%H\"))\n",
    "    dataframe['datetime'] = _date    \n",
    "\n",
    "def get_closest_datestring(datestring='2022-07-13-15',dates=None):\n",
    "    date = datetime.datetime.strptime(datestring, \"%Y-%m-%d-%H\")\n",
    "    #dates = anomalyEnc2.anomalies['date']\n",
    "    _dt = min(dates, key=lambda d: abs(d - date))\n",
    "    _date = make_2digits([_dt.date().month, _dt.date().day, _dt.time().hour])\n",
    "    return  str(_dt.date().year) + '-' + str(_date[0])  + '-' + str(_date[1]) + '-' + str(_date[2])\n",
    "        \n",
    "def filter_dataframe(dataframe,contains=None, date_from=None, date_to=None):        \n",
    "    if contains is not None:\n",
    "        dataframe1 = dataframe[dataframe.index.str.contains(contains)]\n",
    "    elif date_from is not None and date_to is not None:\n",
    "        _from_index = dataframe.index.get_loc(date_from)\n",
    "        _to_index   = dataframe.index.get_loc(date_to)\n",
    "        dataframe1 = dataframe.iloc[_from_index:_to_index + 1]   \n",
    "    return dataframe1\n",
    "    \n",
    "def plot_error(anomalyEnc=None, score=None, anomalies=None, contains=None, date_from=None, date_to=None):\n",
    "    fig = plt.figure(figsize=(18,9))\n",
    "    #_test = anomalyEnc.test_score_df.iloc[2050:]\n",
    "    \n",
    "    if score is None:\n",
    "        _test = anomalyEnc.test_score_df\n",
    "    else:\n",
    "        _test = score\n",
    "    \n",
    "    if anomalies is None:\n",
    "        _anomalies = anomalyEnc.anomalies\n",
    "    else:\n",
    "        _anomalies = anomalies\n",
    "    \n",
    "    date_from = get_closest_datestring(datestring=date_from,dates=_test['datetime'])\n",
    "    date_to = get_closest_datestring(datestring=date_to,dates=_test['datetime'])\n",
    "    \n",
    "    _test = filter_dataframe(_test,contains=contains, date_from=date_from, date_to=date_to)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    _threshold= pd.unique(_anomalies['threshold'])[0]\n",
    "    \n",
    "    plt.axhline(y=_threshold, color=\"red\",linewidth = 3) \n",
    "    plt.plot(_test.index, _test['loss'])   \n",
    "    \n",
    "    \n",
    "    #ticks = (_test.index[0], _test.index[len(_test.index) - 1])\n",
    "    #plt.xticks(rotation=80,ticks=ticks)\n",
    "    skip_x = label_skip(len(_test.index))\n",
    "    plt.xticks( _test.index[0::skip_x],rotation=70,fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    plt_add_anomaly(_test)\n",
    "    \n",
    "    # monthly lines\n",
    "    for timestring in _test[_test.index.str.contains('-01-00')].index:\n",
    "        plt.axvline(x=timestring, color=\"grey\")\n",
    "    \n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_threshold_for_error_in_timerange(enc,date_from='2022-07-12-10',date_to='2022-07-13-15'):\n",
    "    _test = filter_dataframe(enc.test_score_df,date_from='2022-07-12-10',date_to='2022-07-13-15')\n",
    "    _threshold = _test.iloc[0]['loss']\n",
    "    _anomalies = get_anomalies(enc,_threshold).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_thresholds():\n",
    "    update_threshold_for_error_in_timerange(anomalyEnc_hour,date_from='2022-07-12-10',date_to='2022-07-13-15')\n",
    "    update_threshold_for_error_in_timerange(anomalyEnc_CMESSAGETAT2_hour,date_from='2022-07-12-10',date_to='2022-07-13-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE anomalies, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_errors():\n",
    "    _path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_hour/'\n",
    "    _enc = anomalyEnc_hour\n",
    "    _enc.anomalies.to_parquet(_path + 'anomalies.parquet')\n",
    "    _enc.test_score_df.to_parquet(_path + 'test_score_df.parquet')\n",
    "\n",
    "    _path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_CMESSAGETAT2_hour/'\n",
    "    _enc = anomalyEnc_CMESSAGETAT2_hour\n",
    "    _enc.anomalies.to_parquet(_path + 'anomalies.parquet')\n",
    "    _enc.test_score_df.to_parquet(_path + 'test_score_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bounding_lines(timerange,color=\"red\"):\n",
    "    hours= False\n",
    "    if len(timerange[0].split('-')) == 4:\n",
    "        hours = True\n",
    "    if hours == False:\n",
    "        plt.axvline(x=timerange[0], color=color)\n",
    "    else:    \n",
    "        plt.axvline(x=timerange[0], color=color)\n",
    "        plt.axvline(x=timerange[1], color=color)\n",
    "\n",
    "def plot_bounding_box(timerange,_test,scaler):        \n",
    "    e1=_test.index.get_loc(timerange[0])\n",
    "    e2=_test.index.get_loc(timerange[1])\n",
    "    x = np.arange(e1,e2)\n",
    "    _max = np.max(scaler.inverse_transform( _test))\n",
    "    y1 = [0]*len(x)\n",
    "    y2 = [_max]*len(x)\n",
    "    plt.fill_between(x, y1, y2, facecolor='g', alpha=.3)\n",
    "   \n",
    "\n",
    "def plot_anomalies(anomalyEnc=None,test=None, scaler=None, anomalies=None, expect=None, column_y='outcome',contains=None, date_from=None, date_to=None):\n",
    "   \n",
    "    if anomalies is None:\n",
    "        _anomalies = anomalyEnc.anomalies\n",
    "    else:\n",
    "        _anomalies = anomalies\n",
    "    \n",
    "    _threshold= pd.unique(_anomalies['threshold'])[0]\n",
    "    \n",
    "    _from = get_closest_datestring(datestring=date_from, dates = _anomalies['datetime'])\n",
    "    _to = get_closest_datestring(datestring=date_to,dates = _anomalies['datetime'])\n",
    "    _anomalies = filter_dataframe(_anomalies, date_from=_from, date_to=_to)\n",
    "    \n",
    "    if test is None:\n",
    "        _test = anomalyEnc.test\n",
    "    else:\n",
    "        _test = test\n",
    "        \n",
    "    if scaler is None:\n",
    "        _scaler = anomalyEnc.scaler\n",
    "    else:\n",
    "        _scaler = scaler    \n",
    "        \n",
    "    _test = filter_dataframe(_test,contains=contains, date_from=get_closest_datestring(datestring=date_from, dates = _test['datetime']), date_to=get_closest_datestring(datestring=date_to, dates = _test['datetime']))     \n",
    "    #del _test[\"datetime\"]    \n",
    "    _test = _test[['close']]    \n",
    "        \n",
    "    fig = plt.figure(figsize=(18,9))\n",
    "    #fig.set_facecolor('#F2F2F2')\n",
    "    \n",
    "    \n",
    "    if expect is not None:\n",
    "        plot_bounding_box(expect, _test, _scaler)\n",
    "        #plot_bounding_lines(expect)\n",
    "    \n",
    "    \n",
    "    #plt.plot(_test.index, _test[column_y])       \n",
    "    plt.plot(\n",
    "      _test.index,  \n",
    "      _scaler.inverse_transform( _test),   \n",
    "      label='msg count'\n",
    "    );\n",
    "\n",
    "    _a = inverse_transform_anomalies(scaler=_scaler,anomalies=_anomalies, column_y='close')\n",
    "    ax = scatterplot(_anomalies.index, _a, 'anomaly') \n",
    "    ax.set_facecolor('#F2F2F2')   # set the background color of the plot area\n",
    "    \n",
    "    skip_x = label_skip(len(_test.index))\n",
    "    plt.xticks( _test.index[0::skip_x],rotation=70,fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    # monthly lines\n",
    "    for timestring in _test[_test.index.str.contains('-01-00')].index:\n",
    "        plt.axvline(x=timestring, color=\"grey\")   \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_test = plot_anomalies(enc,date_from='2022-07-08-10',date_to='2022-07-14-15',expect=('2022-07-12-10','2022-07-13-15'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -l /home/jovyan/work/output/experiment_anomaly_expect/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sender):\n",
    "    pfall = df[df['CSENDERENDPOINTID'] == sender]\n",
    "    #print('1')\n",
    "    create_data(pfall)\n",
    "    #print('2')\n",
    "    train_models()\n",
    "    save_models()\n",
    "    update_thresholds()\n",
    "    save_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global sender\n",
    "#sender = senders[2]\n",
    "#process(sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_hour/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def listdirectory(path=None,filter='.'):\n",
    "    return [x for x in listdir(path) if not x.startswith(filter)]    \n",
    "\n",
    "_path = '/home/jovyan/work/output/experiment_anomaly_expect/'\n",
    "_files = listdirectory(path=_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sender in senders:\n",
    "#    if str(sender) not in _files:\n",
    "#        process(sender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "\n",
    "def listdirectory(path=None,filter='.'):\n",
    "    return [x for x in listdir(path) if not x.startswith(filter)]    \n",
    "\n",
    "path = '/home/jovyan/work/output/experiment_anomaly_expect/' \n",
    "senders = listdirectory(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_files)\n",
    "#_files[0], senders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomalies.parquet  scaler.pkl  test.parquet  test_score_df.parquet\n"
     ]
    }
   ],
   "source": [
    "#!ls /home/jovyan/work/output/experiment_anomaly_expect/2580/anomalyEnc_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = senders[10]\n",
    "\n",
    "from pickle import load\n",
    "def load_data(experiment=None,path='/home/jovyan/work/output/experiment_anomaly_expect/',folder='anomalyEnc_CMESSAGETAT2_hour'):\n",
    "    global scaler,anomalies,test,score\n",
    "    _path = path + experiment + '/' + folder + '/'\n",
    "    #_path = '/home/jovyan/work/output/experiment_anomaly_expect/' + str(sender) + '/anomalyEnc_hour/'\n",
    "    scaler = load(open(_path + 'scaler.pkl', 'rb'))\n",
    "    anomalies = pd.read_parquet(_path + 'anomalies.parquet')\n",
    "    test  = pd.read_parquet(_path + 'test.parquet')\n",
    "    score = pd.read_parquet(_path + 'test_score_df.parquet')\n",
    "    return scaler,anomalies,test,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_and_anomalies_with_threshold(threshold,score):\n",
    "    score2 = score.copy()\n",
    "    score2['threshold'] = threshold\n",
    "    score2['anomaly'] = score2.loss >= score2.threshold\n",
    "    anomalies2 = score2[score2.anomaly == True]\n",
    "    add_datetime_column(anomalies2)\n",
    "    return score2,anomalies2\n",
    "\n",
    "def create_date_cluster(anomalies3):\n",
    "    month_datelist = list(anomalies3['datetime'])\n",
    "    _start=month_datelist[0].date().day\n",
    "    _current = _start\n",
    "    _count = 0\n",
    "    _results=[]\n",
    "    for date in month_datelist :\n",
    "        if date.date().day - _current > 1:\n",
    "                _results.append((_start,_current,_count))\n",
    "                _start = date.date().day\n",
    "                _count = 0\n",
    "        _current = date.date().day  \n",
    "        _count = _count + 1\n",
    "    _results.append((_start,_current,_count))   \n",
    "    return _results\n",
    "\n",
    "def get_score_region(score):\n",
    "    _from = get_closest_datestring(datestring='2022-07-12-10', dates = score['datetime'])\n",
    "    _to = get_closest_datestring(datestring='2022-07-13-15',dates = score['datetime'])\n",
    "    _score = filter_dataframe(score, date_from=_from, date_to=_to)\n",
    "    return _score\n",
    "\n",
    "def get_selected_thresholds(_score):\n",
    "    _score['iclose'] = scaler.inverse_transform( _score[['close']]) \n",
    "    _iscore = _score[_score['iclose'] > 0.0000001]\n",
    "    _min = min(_iscore.loss)\n",
    "    _max = max(_iscore.loss)\n",
    "    _avg = (_max - _min)/2 + _min\n",
    "    return _min,_max,_avg\n",
    "\n",
    "def get_number_anomalies_with_threshold(_min,score):\n",
    "    score2,anomalies2 = get_score_and_anomalies_with_threshold(_min,score)\n",
    "    anomalies2['iclose'] = scaler.inverse_transform( anomalies2[['close']]) \n",
    "    anomalies3 = anomalies2[anomalies2.index.str.contains('2022-07')]\n",
    "    cluster = create_date_cluster(anomalies3)\n",
    "    return len(anomalies2), len(anomalies2[anomalies2['iclose'] > 0.0000001]), len(anomalies3), len(anomalies3[anomalies3['iclose'] > 0.0000001]), cluster \n",
    "\n",
    "\n",
    "def get_selected_number_anomalies(score):\n",
    "    try:\n",
    "        add_datetime_column(score)\n",
    "        _score = get_score_region(score)\n",
    "        _min,_max,_avg = get_selected_thresholds(_score)\n",
    "        _lmin = get_number_anomalies_with_threshold(_min,score)\n",
    "        _lmax = get_number_anomalies_with_threshold(_max,score)\n",
    "        _lavg = get_number_anomalies_with_threshold(_avg,score)\n",
    "        return _lmin,_lmax,_lavg,  len(get_score_and_anomalies_with_threshold(_score.loss[0],score)[1])\n",
    "    except Exception as exception:\n",
    "        #print(exception)\n",
    "        return (0,0),(0,0),(0,0),  0\n",
    "    \n",
    "    #_min = min(_score.loss)\n",
    "    #_max = max(_score.loss)\n",
    "    #return len(get_score_and_anomalies_with_threshold(_min,_score)[1]), len(get_score_and_anomalies_with_threshold(_max,_score)[1]), len(get_score_and_anomalies_with_threshold(_score.loss[0],_score)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "#pd.options.mode.chained_assignment = \"warn\"\n",
    "\n",
    "def create_statistics(senders,folder='anomalyEnc_CMESSAGETAT2_hour'):\n",
    "    global statistics\n",
    "    statistics = pd.DataFrame(columns=['min0','max0','avg0','start', 'min','max','avg', 'min_07','max_07','avg_07', 'ccmin_07','ccmax_07','ccavg_07','cmin_07','cmax_07','cavg_07', 'amin','amax','astart','test_not_null','filled','values'])\n",
    "\n",
    "    for sender in senders:\n",
    "        try:\n",
    "            scaler,anomalies,test,score = load_data(experiment=str(sender),folder=folder)\n",
    "            _lmin,_lmax,_lavg, _lstart = get_selected_number_anomalies(score)\n",
    "            \n",
    "            _result =scaler.inverse_transform( test[['close']]) \n",
    "            _values_test_not_null = len(_result[_result > 0.0000001])\n",
    "            _filled = round(1 - _values_test_not_null / len(_result),3)\n",
    "\n",
    "            add_datetime_column(score)\n",
    "            _region = get_score_region(score)\n",
    "            _result = scaler.inverse_transform( _region[['close']])\n",
    "            #values = len(_result[_result != 0])\n",
    "            values = len(_result[_result > 0.0000001])\n",
    "\n",
    "            statistics.loc[sender] = _lmin[0],_lmax[0],_lavg[0],_lstart, _lmin[1],_lmax[1],_lavg[1],_lmin[3],_lmax[3],_lavg[3],\\\n",
    "            len(_lmin[4]),len(_lmax[4]),len(_lavg[4]), _lmin[4],_lmax[4],_lavg[4],\\\n",
    "            _lmin[0]/len(test),_lmax[0]/len(test),_lavg[0]/len(test),_values_test_not_null,_filled,values\n",
    "        except Exception as exception: \n",
    "            print(exception)\n",
    "            pass\n",
    "        \n",
    "    statistics['min'] = statistics['min'].astype(int)\n",
    "    statistics['max'] = statistics['max'].astype(int)\n",
    "    statistics['avg'] = statistics['avg'].astype(int)\n",
    "    statistics['min0'] = statistics['min0'].astype(int)\n",
    "    statistics['max0'] = statistics['max0'].astype(int)\n",
    "    statistics['avg0'] = statistics['avg0'].astype(int)\n",
    "    statistics['start'] = statistics['start'].astype(int) \n",
    "    statistics['values'] = statistics['values'].astype(int)  \n",
    "    statistics['test_not_null'] = statistics['test_not_null'].astype(int)\n",
    "    return statistics       \n",
    "\n",
    "#_statistics1 = create_statistics(senders)\n",
    "#_statistics.to_parquet('/home/jovyan/work/output/statistics2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_statistics1 = create_statistics(senders,folder='anomalyEnc_CMESSAGETAT2_hour')\n",
    "_statistics1.to_parquet('/home/jovyan/work/output/statistics_CMESSAGETAT2_hour.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_statistics2 = create_statistics(senders,folder='anomalyEnc_hour')\n",
    "_statistics2.to_parquet('/home/jovyan/work/output/statistics_anomalyEnc_hour.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
